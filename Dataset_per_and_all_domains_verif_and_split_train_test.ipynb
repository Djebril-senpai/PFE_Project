{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fd008b",
   "metadata": {},
   "source": [
    "# **Administration interne et pilotage (SG, RH, achats, systèmes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dfca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1497cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes: 750\n",
      "Clusters uniques: 250\n",
      "\n",
      "--- Lignes par domaine ---\n",
      "  750  Administration interne et pilotage (SG, RH, achats, systèmes)\n",
      "\n",
      "--- Lignes par pair_type ---\n",
      "  250  def\n",
      "  250  context\n",
      "  250  qa\n",
      "\n",
      "--- Lignes par anchor_type ---\n",
      "  375  acronym\n",
      "  375  term\n",
      "\n",
      "--- Clusters incomplets / non standards ---\n",
      "OK: tous les clusters ont def/context/qa\n",
      "\n",
      "--- Doublons exacts ---\n",
      "0 doublon(s)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"dataset_domaine1_admin_interne.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Total lignes: {len(rows)}\")\n",
    "\n",
    "# Clusters uniques\n",
    "clusters = {r[\"cluster_id\"] for r in rows}\n",
    "print(f\"Clusters uniques: {len(clusters)}\")\n",
    "\n",
    "# Répartition par domaine / pair_type / anchor_type\n",
    "by_domain = Counter(r[\"domain\"] for r in rows)\n",
    "by_pair_type = Counter(r[\"pair_type\"] for r in rows)\n",
    "by_anchor_type = Counter(r[\"anchor_type\"] for r in rows)\n",
    "\n",
    "print(\"\\n--- Lignes par domaine ---\")\n",
    "for dom, n in by_domain.most_common():\n",
    "    print(f\"{n:5d}  {dom}\")\n",
    "\n",
    "print(\"\\n--- Lignes par pair_type ---\")\n",
    "for p, n in by_pair_type.most_common():\n",
    "    print(f\"{n:5d}  {p}\")\n",
    "\n",
    "print(\"\\n--- Lignes par anchor_type ---\")\n",
    "for a, n in by_anchor_type.most_common():\n",
    "    print(f\"{n:5d}  {a}\")\n",
    "\n",
    "# Vérif: chaque cluster doit idéalement avoir def/context/qa (3 lignes)\n",
    "cluster_pairs = defaultdict(set)\n",
    "for r in rows:\n",
    "    cluster_pairs[r[\"cluster_id\"]].add(r[\"pair_type\"])\n",
    "\n",
    "incomplete = {cid: sorted(list(pairs)) for cid, pairs in cluster_pairs.items() if set(pairs) != {\"def\", \"context\", \"qa\"}}\n",
    "\n",
    "print(\"\\n--- Clusters incomplets / non standards ---\")\n",
    "if not incomplete:\n",
    "    print(\"OK: tous les clusters ont def/context/qa\")\n",
    "else:\n",
    "    for cid, pairs in sorted(incomplete.items()):\n",
    "        print(f\"{cid}: {pairs}\")\n",
    "\n",
    "# Doublons exacts (même cluster_id, pair_type, anchor, positive)\n",
    "seen = set()\n",
    "dups = []\n",
    "for r in rows:\n",
    "    key = (r.get(\"cluster_id\"), r.get(\"pair_type\"), r.get(\"anchor\"), r.get(\"positive\"))\n",
    "    if key in seen:\n",
    "        dups.append(key)\n",
    "    else:\n",
    "        seen.add(key)\n",
    "\n",
    "print(\"\\n--- Doublons exacts ---\")\n",
    "print(f\"{len(dups)} doublon(s)\")\n",
    "if dups[:5]:\n",
    "    print(\"Exemples:\", dups[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d631e",
   "metadata": {},
   "source": [
    "# **Juridique public et commande publique (DAJ, marchés publics, contentieux)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ede9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes: 750\n",
      "Clusters uniques: 250\n",
      "\n",
      "--- Lignes par domaine ---\n",
      "  750  Juridique public et commande publique (DAJ, marchés publics, contentieux)\n",
      "\n",
      "--- Lignes par pair_type ---\n",
      "  250  def\n",
      "  250  context\n",
      "  250  qa\n",
      "\n",
      "--- Lignes par anchor_type ---\n",
      "  375  acronym\n",
      "  375  term\n",
      "\n",
      "--- Clusters incomplets / non standards ---\n",
      "OK: tous les clusters ont def/context/qa\n",
      "\n",
      "--- Doublons exacts ---\n",
      "0 doublon(s)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"dataset_domaine2_juridique.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Total lignes: {len(rows)}\")\n",
    "\n",
    "# Clusters uniques\n",
    "clusters = {r[\"cluster_id\"] for r in rows}\n",
    "print(f\"Clusters uniques: {len(clusters)}\")\n",
    "\n",
    "# Répartition par domaine / pair_type / anchor_type\n",
    "by_domain = Counter(r[\"domain\"] for r in rows)\n",
    "by_pair_type = Counter(r[\"pair_type\"] for r in rows)\n",
    "by_anchor_type = Counter(r[\"anchor_type\"] for r in rows)\n",
    "\n",
    "print(\"\\n--- Lignes par domaine ---\")\n",
    "for dom, n in by_domain.most_common():\n",
    "    print(f\"{n:5d}  {dom}\")\n",
    "\n",
    "print(\"\\n--- Lignes par pair_type ---\")\n",
    "for p, n in by_pair_type.most_common():\n",
    "    print(f\"{n:5d}  {p}\")\n",
    "\n",
    "print(\"\\n--- Lignes par anchor_type ---\")\n",
    "for a, n in by_anchor_type.most_common():\n",
    "    print(f\"{n:5d}  {a}\")\n",
    "\n",
    "# Vérif: chaque cluster doit idéalement avoir def/context/qa (3 lignes)\n",
    "cluster_pairs = defaultdict(set)\n",
    "for r in rows:\n",
    "    cluster_pairs[r[\"cluster_id\"]].add(r[\"pair_type\"])\n",
    "\n",
    "incomplete = {cid: sorted(list(pairs)) for cid, pairs in cluster_pairs.items() if set(pairs) != {\"def\", \"context\", \"qa\"}}\n",
    "\n",
    "print(\"\\n--- Clusters incomplets / non standards ---\")\n",
    "if not incomplete:\n",
    "    print(\"OK: tous les clusters ont def/context/qa\")\n",
    "else:\n",
    "    for cid, pairs in sorted(incomplete.items()):\n",
    "        print(f\"{cid}: {pairs}\")\n",
    "\n",
    "# Doublons exacts (même cluster_id, pair_type, anchor, positive)\n",
    "seen = set()\n",
    "dups = []\n",
    "for r in rows:\n",
    "    key = (r.get(\"cluster_id\"), r.get(\"pair_type\"), r.get(\"anchor\"), r.get(\"positive\"))\n",
    "    if key in seen:\n",
    "        dups.append(key)\n",
    "    else:\n",
    "        seen.add(key)\n",
    "\n",
    "print(\"\\n--- Doublons exacts ---\")\n",
    "print(f\"{len(dups)} doublon(s)\")\n",
    "if dups[:5]:\n",
    "    print(\"Exemples:\", dups[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38fcc2b",
   "metadata": {},
   "source": [
    "# **Fiscalité et recouvrement (DGFiP, DLF, contrôle, procédures)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c26084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes: 750\n",
      "Clusters uniques: 250\n",
      "\n",
      "--- Lignes par domaine ---\n",
      "  750  Fiscalité et recouvrement (DGFiP, DLF, contrôle, procédures)\n",
      "\n",
      "--- Lignes par pair_type ---\n",
      "  250  def\n",
      "  250  context\n",
      "  250  qa\n",
      "\n",
      "--- Lignes par anchor_type ---\n",
      "  375  acronym\n",
      "  375  term\n",
      "\n",
      "--- Clusters incomplets / non standards ---\n",
      "OK: tous les clusters ont def/context/qa\n",
      "\n",
      "--- Doublons exacts ---\n",
      "0 doublon(s)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"dataset_domaine3_fiscalite.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Total lignes: {len(rows)}\")\n",
    "\n",
    "# Clusters uniques\n",
    "clusters = {r[\"cluster_id\"] for r in rows}\n",
    "print(f\"Clusters uniques: {len(clusters)}\")\n",
    "\n",
    "# Répartition par domaine / pair_type / anchor_type\n",
    "by_domain = Counter(r[\"domain\"] for r in rows)\n",
    "by_pair_type = Counter(r[\"pair_type\"] for r in rows)\n",
    "by_anchor_type = Counter(r[\"anchor_type\"] for r in rows)\n",
    "\n",
    "print(\"\\n--- Lignes par domaine ---\")\n",
    "for dom, n in by_domain.most_common():\n",
    "    print(f\"{n:5d}  {dom}\")\n",
    "\n",
    "print(\"\\n--- Lignes par pair_type ---\")\n",
    "for p, n in by_pair_type.most_common():\n",
    "    print(f\"{n:5d}  {p}\")\n",
    "\n",
    "print(\"\\n--- Lignes par anchor_type ---\")\n",
    "for a, n in by_anchor_type.most_common():\n",
    "    print(f\"{n:5d}  {a}\")\n",
    "\n",
    "# Vérif: chaque cluster doit idéalement avoir def/context/qa (3 lignes)\n",
    "cluster_pairs = defaultdict(set)\n",
    "for r in rows:\n",
    "    cluster_pairs[r[\"cluster_id\"]].add(r[\"pair_type\"])\n",
    "\n",
    "incomplete = {cid: sorted(list(pairs)) for cid, pairs in cluster_pairs.items() if set(pairs) != {\"def\", \"context\", \"qa\"}}\n",
    "\n",
    "print(\"\\n--- Clusters incomplets / non standards ---\")\n",
    "if not incomplete:\n",
    "    print(\"OK: tous les clusters ont def/context/qa\")\n",
    "else:\n",
    "    for cid, pairs in sorted(incomplete.items()):\n",
    "        print(f\"{cid}: {pairs}\")\n",
    "\n",
    "# Doublons exacts (même cluster_id, pair_type, anchor, positive)\n",
    "seen = set()\n",
    "dups = []\n",
    "for r in rows:\n",
    "    key = (r.get(\"cluster_id\"), r.get(\"pair_type\"), r.get(\"anchor\"), r.get(\"positive\"))\n",
    "    if key in seen:\n",
    "        dups.append(key)\n",
    "    else:\n",
    "        seen.add(key)\n",
    "\n",
    "print(\"\\n--- Doublons exacts ---\")\n",
    "print(f\"{len(dups)} doublon(s)\")\n",
    "if dups[:5]:\n",
    "    print(\"Exemples:\", dups[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78255154",
   "metadata": {},
   "source": [
    "# **Budget, comptabilité publique et exécution (DB, AE/CP, Chorus)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67472d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes: 750\n",
      "Clusters uniques: 250\n",
      "\n",
      "--- Lignes par domaine ---\n",
      "  750  Budget, comptabilité publique et exécution (DB, AE/CP, Chorus)\n",
      "\n",
      "--- Lignes par pair_type ---\n",
      "  250  def\n",
      "  250  context\n",
      "  250  qa\n",
      "\n",
      "--- Lignes par anchor_type ---\n",
      "  375  acronym\n",
      "  375  term\n",
      "\n",
      "--- Clusters incomplets / non standards ---\n",
      "OK: tous les clusters ont def/context/qa\n",
      "\n",
      "--- Doublons exacts ---\n",
      "0 doublon(s)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"dataset_domaine4_budget.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Total lignes: {len(rows)}\")\n",
    "\n",
    "# Clusters uniques\n",
    "clusters = {r[\"cluster_id\"] for r in rows}\n",
    "print(f\"Clusters uniques: {len(clusters)}\")\n",
    "\n",
    "# Répartition par domaine / pair_type / anchor_type\n",
    "by_domain = Counter(r[\"domain\"] for r in rows)\n",
    "by_pair_type = Counter(r[\"pair_type\"] for r in rows)\n",
    "by_anchor_type = Counter(r[\"anchor_type\"] for r in rows)\n",
    "\n",
    "print(\"\\n--- Lignes par domaine ---\")\n",
    "for dom, n in by_domain.most_common():\n",
    "    print(f\"{n:5d}  {dom}\")\n",
    "\n",
    "print(\"\\n--- Lignes par pair_type ---\")\n",
    "for p, n in by_pair_type.most_common():\n",
    "    print(f\"{n:5d}  {p}\")\n",
    "\n",
    "print(\"\\n--- Lignes par anchor_type ---\")\n",
    "for a, n in by_anchor_type.most_common():\n",
    "    print(f\"{n:5d}  {a}\")\n",
    "\n",
    "# Vérif: chaque cluster doit idéalement avoir def/context/qa (3 lignes)\n",
    "cluster_pairs = defaultdict(set)\n",
    "for r in rows:\n",
    "    cluster_pairs[r[\"cluster_id\"]].add(r[\"pair_type\"])\n",
    "\n",
    "incomplete = {cid: sorted(list(pairs)) for cid, pairs in cluster_pairs.items() if set(pairs) != {\"def\", \"context\", \"qa\"}}\n",
    "\n",
    "print(\"\\n--- Clusters incomplets / non standards ---\")\n",
    "if not incomplete:\n",
    "    print(\"OK: tous les clusters ont def/context/qa\")\n",
    "else:\n",
    "    for cid, pairs in sorted(incomplete.items()):\n",
    "        print(f\"{cid}: {pairs}\")\n",
    "\n",
    "# Doublons exacts (même cluster_id, pair_type, anchor, positive)\n",
    "seen = set()\n",
    "dups = []\n",
    "for r in rows:\n",
    "    key = (r.get(\"cluster_id\"), r.get(\"pair_type\"), r.get(\"anchor\"), r.get(\"positive\"))\n",
    "    if key in seen:\n",
    "        dups.append(key)\n",
    "    else:\n",
    "        seen.add(key)\n",
    "\n",
    "print(\"\\n--- Doublons exacts ---\")\n",
    "print(f\"{len(dups)} doublon(s)\")\n",
    "if dups[:5]:\n",
    "    print(\"Exemples:\", dups[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312a29d8",
   "metadata": {},
   "source": [
    "# **Trésor, dette, financement et douanes/anti-fraude (DG_Trésor, AFT, DGDDI, Tracfin)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51331f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes: 750\n",
      "Clusters uniques: 250\n",
      "\n",
      "--- Lignes par domaine ---\n",
      "  750  Trésor, dette, financement et douanes/anti-fraude (DG_Trésor, AFT, DGDDI, Tracfin)\n",
      "\n",
      "--- Lignes par pair_type ---\n",
      "  250  def\n",
      "  250  context\n",
      "  250  qa\n",
      "\n",
      "--- Lignes par anchor_type ---\n",
      "  375  acronym\n",
      "  375  term\n",
      "\n",
      "--- Clusters incomplets / non standards ---\n",
      "OK: tous les clusters ont def/context/qa\n",
      "\n",
      "--- Doublons exacts ---\n",
      "0 doublon(s)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"dataset_domaine5_tresor_douanes.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Total lignes: {len(rows)}\")\n",
    "\n",
    "# Clusters uniques\n",
    "clusters = {r[\"cluster_id\"] for r in rows}\n",
    "print(f\"Clusters uniques: {len(clusters)}\")\n",
    "\n",
    "# Répartition par domaine / pair_type / anchor_type\n",
    "by_domain = Counter(r[\"domain\"] for r in rows)\n",
    "by_pair_type = Counter(r[\"pair_type\"] for r in rows)\n",
    "by_anchor_type = Counter(r[\"anchor_type\"] for r in rows)\n",
    "\n",
    "print(\"\\n--- Lignes par domaine ---\")\n",
    "for dom, n in by_domain.most_common():\n",
    "    print(f\"{n:5d}  {dom}\")\n",
    "\n",
    "print(\"\\n--- Lignes par pair_type ---\")\n",
    "for p, n in by_pair_type.most_common():\n",
    "    print(f\"{n:5d}  {p}\")\n",
    "\n",
    "print(\"\\n--- Lignes par anchor_type ---\")\n",
    "for a, n in by_anchor_type.most_common():\n",
    "    print(f\"{n:5d}  {a}\")\n",
    "\n",
    "# Vérif: chaque cluster doit idéalement avoir def/context/qa (3 lignes)\n",
    "cluster_pairs = defaultdict(set)\n",
    "for r in rows:\n",
    "    cluster_pairs[r[\"cluster_id\"]].add(r[\"pair_type\"])\n",
    "\n",
    "incomplete = {cid: sorted(list(pairs)) for cid, pairs in cluster_pairs.items() if set(pairs) != {\"def\", \"context\", \"qa\"}}\n",
    "\n",
    "print(\"\\n--- Clusters incomplets / non standards ---\")\n",
    "if not incomplete:\n",
    "    print(\"OK: tous les clusters ont def/context/qa\")\n",
    "else:\n",
    "    for cid, pairs in sorted(incomplete.items()):\n",
    "        print(f\"{cid}: {pairs}\")\n",
    "\n",
    "# Doublons exacts (même cluster_id, pair_type, anchor, positive)\n",
    "seen = set()\n",
    "dups = []\n",
    "for r in rows:\n",
    "    key = (r.get(\"cluster_id\"), r.get(\"pair_type\"), r.get(\"anchor\"), r.get(\"positive\"))\n",
    "    if key in seen:\n",
    "        dups.append(key)\n",
    "    else:\n",
    "        seen.add(key)\n",
    "\n",
    "print(\"\\n--- Doublons exacts ---\")\n",
    "print(f\"{len(dups)} doublon(s)\")\n",
    "if dups[:5]:\n",
    "    print(\"Exemples:\", dups[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec476c",
   "metadata": {},
   "source": [
    "# **Dataset baconnier (domaine administratif multi-termes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91f0d2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes: 750\n",
      "Clusters uniques: 250\n",
      "\n",
      "--- Lignes par domaine ---\n",
      "  750  Administratif & juridique (dataset Baconnier)\n",
      "\n",
      "--- Lignes par pair_type ---\n",
      "  250  def\n",
      "  250  context\n",
      "  250  qa\n",
      "\n",
      "--- Lignes par anchor_type ---\n",
      "  375  acronym\n",
      "  375  term\n",
      "\n",
      "--- Clusters incomplets / non standards ---\n",
      "OK: tous les clusters ont def/context/qa\n",
      "\n",
      "--- Doublons exacts ---\n",
      "0 doublon(s)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"dataset_domaine6_administratif_v3.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Total lignes: {len(rows)}\")\n",
    "\n",
    "# Clusters uniques\n",
    "clusters = {r[\"cluster_id\"] for r in rows}\n",
    "print(f\"Clusters uniques: {len(clusters)}\")\n",
    "\n",
    "# Répartition par domaine / pair_type / anchor_type\n",
    "by_domain = Counter(r[\"domain\"] for r in rows)\n",
    "by_pair_type = Counter(r[\"pair_type\"] for r in rows)\n",
    "by_anchor_type = Counter(r[\"anchor_type\"] for r in rows)\n",
    "\n",
    "print(\"\\n--- Lignes par domaine ---\")\n",
    "for dom, n in by_domain.most_common():\n",
    "    print(f\"{n:5d}  {dom}\")\n",
    "\n",
    "print(\"\\n--- Lignes par pair_type ---\")\n",
    "for p, n in by_pair_type.most_common():\n",
    "    print(f\"{n:5d}  {p}\")\n",
    "\n",
    "print(\"\\n--- Lignes par anchor_type ---\")\n",
    "for a, n in by_anchor_type.most_common():\n",
    "    print(f\"{n:5d}  {a}\")\n",
    "\n",
    "# Vérif: chaque cluster doit idéalement avoir def/context/qa (3 lignes)\n",
    "cluster_pairs = defaultdict(set)\n",
    "for r in rows:\n",
    "    cluster_pairs[r[\"cluster_id\"]].add(r[\"pair_type\"])\n",
    "\n",
    "incomplete = {cid: sorted(list(pairs)) for cid, pairs in cluster_pairs.items() if set(pairs) != {\"def\", \"context\", \"qa\"}}\n",
    "\n",
    "print(\"\\n--- Clusters incomplets / non standards ---\")\n",
    "if not incomplete:\n",
    "    print(\"OK: tous les clusters ont def/context/qa\")\n",
    "else:\n",
    "    for cid, pairs in sorted(incomplete.items()):\n",
    "        print(f\"{cid}: {pairs}\")\n",
    "\n",
    "# Doublons exacts (même cluster_id, pair_type, anchor, positive)\n",
    "seen = set()\n",
    "dups = []\n",
    "for r in rows:\n",
    "    key = (r.get(\"cluster_id\"), r.get(\"pair_type\"), r.get(\"anchor\"), r.get(\"positive\"))\n",
    "    if key in seen:\n",
    "        dups.append(key)\n",
    "    else:\n",
    "        seen.add(key)\n",
    "\n",
    "print(\"\\n--- Doublons exacts ---\")\n",
    "print(f\"{len(dups)} doublon(s)\")\n",
    "if dups[:5]:\n",
    "    print(\"Exemples:\", dups[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a6bc2",
   "metadata": {},
   "source": [
    "# **Dataset final : Tous les domaines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2952fa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lignes: 4500\n",
      "Clusters uniques: 1500\n",
      "\n",
      "--- Lignes par domaine ---\n",
      "  750  Administration interne et pilotage (SG, RH, achats, systèmes)\n",
      "  750  Juridique public et commande publique (DAJ, marchés publics, contentieux)\n",
      "  750  Fiscalité et recouvrement (DGFiP, DLF, contrôle, procédures)\n",
      "  750  Budget, comptabilité publique et exécution (DB, AE/CP, Chorus)\n",
      "  750  Trésor, dette, financement et douanes/anti-fraude (DG_Trésor, AFT, DGDDI, Tracfin)\n",
      "  750  Administratif & juridique (dataset Baconnier)\n",
      "\n",
      "--- Lignes par pair_type ---\n",
      " 1500  def\n",
      " 1500  context\n",
      " 1500  qa\n",
      "\n",
      "--- Lignes par anchor_type ---\n",
      " 2250  acronym\n",
      " 2250  term\n",
      "\n",
      "--- Clusters incomplets / non standards ---\n",
      "OK: tous les clusters ont def/context/qa\n",
      "\n",
      "--- Doublons exacts ---\n",
      "0 doublon(s)\n"
     ]
    }
   ],
   "source": [
    "PATH = \"Dataset_Bercy_4k_lines.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "print(f\"Total lignes: {len(rows)}\")\n",
    "\n",
    "# Clusters uniques\n",
    "clusters = {r[\"cluster_id\"] for r in rows}\n",
    "print(f\"Clusters uniques: {len(clusters)}\")\n",
    "\n",
    "# Répartition par domaine / pair_type / anchor_type\n",
    "by_domain = Counter(r[\"domain\"] for r in rows)\n",
    "by_pair_type = Counter(r[\"pair_type\"] for r in rows)\n",
    "by_anchor_type = Counter(r[\"anchor_type\"] for r in rows)\n",
    "\n",
    "print(\"\\n--- Lignes par domaine ---\")\n",
    "for dom, n in by_domain.most_common():\n",
    "    print(f\"{n:5d}  {dom}\")\n",
    "\n",
    "print(\"\\n--- Lignes par pair_type ---\")\n",
    "for p, n in by_pair_type.most_common():\n",
    "    print(f\"{n:5d}  {p}\")\n",
    "\n",
    "print(\"\\n--- Lignes par anchor_type ---\")\n",
    "for a, n in by_anchor_type.most_common():\n",
    "    print(f\"{n:5d}  {a}\")\n",
    "\n",
    "# Vérif: chaque cluster doit idéalement avoir def/context/qa (3 lignes)\n",
    "cluster_pairs = defaultdict(set)\n",
    "for r in rows:\n",
    "    cluster_pairs[r[\"cluster_id\"]].add(r[\"pair_type\"])\n",
    "\n",
    "incomplete = {cid: sorted(list(pairs)) for cid, pairs in cluster_pairs.items() if set(pairs) != {\"def\", \"context\", \"qa\"}}\n",
    "\n",
    "print(\"\\n--- Clusters incomplets / non standards ---\")\n",
    "if not incomplete:\n",
    "    print(\"OK: tous les clusters ont def/context/qa\")\n",
    "else:\n",
    "    for cid, pairs in sorted(incomplete.items()):\n",
    "        print(f\"{cid}: {pairs}\")\n",
    "\n",
    "# Doublons exacts (même cluster_id, pair_type, anchor, positive)\n",
    "seen = set()\n",
    "dups = []\n",
    "for r in rows:\n",
    "    key = (r.get(\"cluster_id\"), r.get(\"pair_type\"), r.get(\"anchor\"), r.get(\"positive\"))\n",
    "    if key in seen:\n",
    "        dups.append(key)\n",
    "    else:\n",
    "        seen.add(key)\n",
    "\n",
    "print(\"\\n--- Doublons exacts ---\")\n",
    "print(f\"{len(dups)} doublon(s)\")\n",
    "if dups[:5]:\n",
    "    print(\"Exemples:\", dups[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6661c09c",
   "metadata": {},
   "source": [
    "# **Split JSONL 90 (train) / 10 (test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f295c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75457992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 4500\n",
      "Unique cluster_id: 1500\n",
      "Target test lines (approx): 450\n",
      "Split done\n",
      "Train rows: 4032 -> bercy_train_90.jsonl\n",
      "Test  rows: 468 -> bercy_test_10.jsonl\n",
      "Overlap cluster_ids (should be 0): 0\n",
      "\n",
      "Train domain counts: {'Administration interne et pilotage (SG, RH, achats, systèmes)': 672, 'Juridique public et commande publique (DAJ, marchés publics, contentieux)': 672, 'Fiscalité et recouvrement (DGFiP, DLF, contrôle, procédures)': 672, 'Budget, comptabilité publique et exécution (DB, AE/CP, Chorus)': 672, 'Trésor, dette, financement et douanes/anti-fraude (DG_Trésor, AFT, DGDDI, Tracfin)': 672, 'Administratif & juridique (dataset Baconnier)': 672}\n",
      "Test domain counts: {'Administration interne et pilotage (SG, RH, achats, systèmes)': 78, 'Juridique public et commande publique (DAJ, marchés publics, contentieux)': 78, 'Fiscalité et recouvrement (DGFiP, DLF, contrôle, procédures)': 78, 'Budget, comptabilité publique et exécution (DB, AE/CP, Chorus)': 78, 'Trésor, dette, financement et douanes/anti-fraude (DG_Trésor, AFT, DGDDI, Tracfin)': 78, 'Administratif & juridique (dataset Baconnier)': 78}\n",
      "\n",
      "Train anchor_type counts: {'acronym': 2016, 'term': 2016}\n",
      "Test anchor_type counts: {'acronym': 234, 'term': 234}\n"
     ]
    }
   ],
   "source": [
    "# CONFIG\n",
    "INPUT_PATH = Path(\"Dataset_Bercy_4k_lines.jsonl\")\n",
    "OUT_TRAIN = Path(\"bercy_train_90.jsonl\")\n",
    "OUT_TEST  = Path(\"bercy_test_10.jsonl\")\n",
    "\n",
    "TEST_RATIO = 0.10\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "\n",
    "# 1) Load rows\n",
    "rows = []\n",
    "with INPUT_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            rows.append(json.loads(line))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(f\"Loaded rows: {len(rows)}\")\n",
    "\n",
    "# 2) Group by cluster_id\n",
    "by_cluster = defaultdict(list)\n",
    "for r in rows:\n",
    "    cid = r.get(\"cluster_id\")\n",
    "    if cid:\n",
    "        by_cluster[cid].append(r)\n",
    "\n",
    "cluster_ids = list(by_cluster.keys())\n",
    "print(f\"Unique cluster_id: {len(cluster_ids)}\")\n",
    "\n",
    "# 3) Build cluster metadata (domain + anchor_type) and stratify\n",
    "# Assume each cluster has consistent domain + anchor_type\n",
    "cluster_meta = {}\n",
    "strata = defaultdict(list)  # (domain, anchor_type) -> [cluster_id,...]\n",
    "cluster_sizes = {}          # cluster_id -> nb_rows\n",
    "\n",
    "for cid, items in by_cluster.items():\n",
    "    domains = [it.get(\"domain\") for it in items if it.get(\"domain\")]\n",
    "    anchors = [it.get(\"anchor_type\") for it in items if it.get(\"anchor_type\")]\n",
    "    if not domains or not anchors:\n",
    "        continue\n",
    "\n",
    "    dom = Counter(domains).most_common(1)[0][0]\n",
    "    at  = Counter(anchors).most_common(1)[0][0]\n",
    "\n",
    "    cluster_meta[cid] = (dom, at)\n",
    "    strata[(dom, at)].append(cid)\n",
    "    cluster_sizes[cid] = len(items)\n",
    "\n",
    "# Target test size in lines\n",
    "target_test_lines = int(len(rows) * TEST_RATIO)\n",
    "print(f\"Target test lines (approx): {target_test_lines}\")\n",
    "\n",
    "# Compute target per domain, then split 50/50 by anchor_type inside domain\n",
    "domains = sorted({d for (d, a) in strata.keys()})\n",
    "n_domains = len(domains)\n",
    "\n",
    "# Ideal: equal domain share in test lines\n",
    "target_per_domain_lines = {d: target_test_lines // n_domains for d in domains}\n",
    "# Distribute remainder\n",
    "rem = target_test_lines - sum(target_per_domain_lines.values())\n",
    "for d in domains[:rem]:\n",
    "    target_per_domain_lines[d] += 1\n",
    "\n",
    "# For each domain, 50% acronym / 50% term (in LINES, approx)\n",
    "def pick_clusters_for_target(candidates, target_lines):\n",
    "    \"\"\"Greedy pick clusters until reaching target_lines (or as close as possible).\"\"\"\n",
    "    random.shuffle(candidates)\n",
    "    picked = []\n",
    "    total = 0\n",
    "    for cid in candidates:\n",
    "        if total >= target_lines:\n",
    "            break\n",
    "        picked.append(cid)\n",
    "        total += cluster_sizes[cid]\n",
    "    return picked, total\n",
    "\n",
    "test_clusters = set()\n",
    "test_lines_count = 0\n",
    "\n",
    "for d in domains:\n",
    "    # candidates per anchor_type\n",
    "    cand_acr = strata.get((d, \"acronym\"), []).copy()\n",
    "    cand_term = strata.get((d, \"term\"), []).copy()\n",
    "\n",
    "    dom_target = target_per_domain_lines[d]\n",
    "    acr_target = dom_target // 2\n",
    "    term_target = dom_target - acr_target\n",
    "\n",
    "    picked_acr, acr_lines = pick_clusters_for_target(cand_acr, acr_target)\n",
    "    picked_term, term_lines = pick_clusters_for_target(cand_term, term_target)\n",
    "\n",
    "    test_clusters.update(picked_acr)\n",
    "    test_clusters.update(picked_term)\n",
    "    test_lines_count += (acr_lines + term_lines)\n",
    "\n",
    "# 4) Build train/test rows\n",
    "train_rows, test_rows = [], []\n",
    "for cid, items in by_cluster.items():\n",
    "    if cid in test_clusters:\n",
    "        test_rows.extend(items)\n",
    "    else:\n",
    "        train_rows.extend(items)\n",
    "\n",
    "# 5) Save\n",
    "def write_jsonl(path: Path, data):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in data:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(OUT_TRAIN, train_rows)\n",
    "write_jsonl(OUT_TEST, test_rows)\n",
    "\n",
    "print(\"Split done\")\n",
    "print(f\"Train rows: {len(train_rows)} -> {OUT_TRAIN}\")\n",
    "print(f\"Test  rows: {len(test_rows)} -> {OUT_TEST}\")\n",
    "\n",
    "# 6) Sanity checks\n",
    "train_cids = {r[\"cluster_id\"] for r in train_rows if \"cluster_id\" in r}\n",
    "test_cids  = {r[\"cluster_id\"] for r in test_rows if \"cluster_id\" in r}\n",
    "overlap = train_cids.intersection(test_cids)\n",
    "print(f\"Overlap cluster_ids (should be 0): {len(overlap)}\")\n",
    "\n",
    "# Domain distribution\n",
    "train_dom = Counter(r.get(\"domain\") for r in train_rows)\n",
    "test_dom  = Counter(r.get(\"domain\") for r in test_rows)\n",
    "print(\"\\nTrain domain counts:\", dict(train_dom))\n",
    "print(\"Test domain counts:\", dict(test_dom))\n",
    "\n",
    "# Anchor_type distribution\n",
    "train_at = Counter(r.get(\"anchor_type\") for r in train_rows)\n",
    "test_at  = Counter(r.get(\"anchor_type\") for r in test_rows)\n",
    "print(\"\\nTrain anchor_type counts:\", dict(train_at))\n",
    "print(\"Test anchor_type counts:\", dict(test_at))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
