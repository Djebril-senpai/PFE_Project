{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26566e61",
   "metadata": {},
   "source": [
    "# **Evaluation des modèles baseline (avant fine-tuning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623a2ea",
   "metadata": {},
   "source": [
    "### *2 versions pour évaluer le dataset :*\n",
    "\n",
    "- Concept (tolérante)\n",
    "- Def-only (stricte)\n",
    "\n",
    "#### *Concept :*\n",
    "\n",
    "But -> “Est-ce que le modèle regroupe bien toutes les formulations du même concept (cluster_id) ?”\n",
    "Exemple : query = “ASE”, Si le top-1 renvoie le contexte lié à ASE au lieu de la def ASE → c’est compté bon.\n",
    "\n",
    "Donc, ca mesure la capacité de clustering sémantique autour d’un concept.\n",
    "\n",
    "#### *Def-only :*\n",
    "\n",
    "But : “Est-ce que le modèle retrouve la bonne définition quand je pose une question ?”\n",
    "Donc cette fois, c'est uniquement le document pair_type=\"def\" du même cluster_id.\n",
    "\n",
    "Exemple : query = “ASE”, Si le top-1 renvoie la context ASE → c’est faux\n",
    "il faut que ça renvoie la définition d'ASE.\n",
    "\n",
    "Ça mesure ce qui est le plus proche d’un RAG propre\n",
    "(ramener une source stable/encyclopédique plutôt qu’une phrase d’usage)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61613e",
   "metadata": {},
   "source": [
    "#### **Evaluation sur le dataset de test (10% du dataset complet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0454d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from typing import Optional\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 468\n",
      "Docs built (def+context): 312\n",
      "Queries built: 780\n",
      "Missing concept GT: 0\n",
      "Missing def-only GT: 0\n",
      "\n",
      "Loading: Solon-Large (OrdalieTech/SOLON-embeddings-large-0.1) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afec01dbb93b4a2d9f6c4e378b0587d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15a0c6ec28b405b98d24d4685d7f5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large | Concept R@1=99.1% | DefOnly R@1=78.1% | Total=0.8s\n",
      "\n",
      "Loading: Solon-Large-FT-Config1 (final_models\\solon_large_finetuned_config1_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config1_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55e55ebb7424e52b4292aaa332a41de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e9aa76e7a0475fb283a1b016ec2c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config1 | Concept R@1=81.9% | DefOnly R@1=50.9% | Total=0.9s\n",
      "\n",
      "Loading: Solon-Large-FT-Config2 (final_models\\solon_large_finetuned_config2_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config2_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84150067a8074307964a1978321dfdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b51ff46efd47d8aed5e680dbf0702b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config2 | Concept R@1=90.5% | DefOnly R@1=51.0% | Total=0.8s\n",
      "\n",
      "Loading: Solon-Large-FT-Config3 (final_models\\solon_large_finetuned_config3_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config3_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4e76b267a742bc9c76c2cd02d71c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018d82ca908f4704bad9f8646a8766f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config3 | Concept R@1=65.1% | DefOnly R@1=40.1% | Total=0.8s\n",
      "\n",
      "Loading: Solon-Large-FT-Config4 (final_models\\solon_large_finetuned_config4_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config4_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cc2a1f0ba148c1be3b4b841bc66a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bb4e50c38243bc97c09add5d2fd46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config4 | Concept R@1=72.8% | DefOnly R@1=37.6% | Total=0.8s\n",
      "\n",
      "Loading: E5-Large (intfloat/multilingual-e5-large) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f232b4b4414924bc284a6c78c54de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404148006c2a4c3c8fa295045140ae84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large | Concept R@1=99.9% | DefOnly R@1=80.0% | Total=0.9s\n",
      "\n",
      "Loading: E5-Large-instruct (intfloat/multilingual-e5-large-instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554416e4667a4e5fa295a5b3d7117646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ad82d994bc4f7a9a281a889d45e058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large-instruct | Concept R@1=100.0% | DefOnly R@1=91.7% | Total=0.9s\n",
      "\n",
      "Loading: E5-Large-FT-Config1 (final_models\\e5_large_finetuned_config1_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\e5_large_finetuned_config1_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d31029663ac43ea94193df335834d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b967133dcc14e629d1a2c16ef4b6620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large-FT-Config1 | Concept R@1=99.6% | DefOnly R@1=88.5% | Total=0.8s\n",
      "\n",
      "Loading: E5-Large-FT-Config2 (final_models\\e5_large_finetuned_config2_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\e5_large_finetuned_config2_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42a6894972a457ea835b23f18fa0c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7ac0d78d2a4613a44b888df744c574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large-FT-Config2 | Concept R@1=98.3% | DefOnly R@1=75.3% | Total=0.9s\n",
      "\n",
      "Loading: E5-Large-FT-Config3 (final_models\\e5_large_finetuned_config3_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\e5_large_finetuned_config3_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf71a45a9a8e46c8a5e2f554a23e912a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abeecef8aaed49879b2264b67028b33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large-FT-Config3 | Concept R@1=99.9% | DefOnly R@1=89.5% | Total=0.8s\n",
      "\n",
      "Loading: E5-Large-FT-Config4 (final_models\\e5_large_finetuned_config4_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\e5_large_finetuned_config4_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679e468f2fe84bc2b350535c1c96c03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0049e432ffb84d6787cd16f215022a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large-FT-Config4 | Concept R@1=99.4% | DefOnly R@1=76.9% | Total=0.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_752e3 th {\n",
       "  background-color: #111111;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_752e3 td {\n",
       "  border-color: #333333;\n",
       "}\n",
       "#T_752e3 caption {\n",
       "  caption-side: top;\n",
       "  color: white;\n",
       "  font-size: 14pt;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_752e3_row0_col0, #T_752e3_row1_col0, #T_752e3_row2_col0, #T_752e3_row3_col0, #T_752e3_row4_col0, #T_752e3_row5_col0, #T_752e3_row6_col0, #T_752e3_row7_col0, #T_752e3_row8_col0, #T_752e3_row9_col0, #T_752e3_row10_col0 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background-color: #1f3c88;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_752e3_row0_col1, #T_752e3_row0_col2, #T_752e3_row0_col3, #T_752e3_row0_col4, #T_752e3_row1_col2, #T_752e3_row1_col3, #T_752e3_row1_col4, #T_752e3_row2_col2, #T_752e3_row2_col3, #T_752e3_row2_col4, #T_752e3_row3_col2, #T_752e3_row3_col3, #T_752e3_row3_col4, #T_752e3_row4_col2, #T_752e3_row4_col3, #T_752e3_row4_col4, #T_752e3_row5_col2, #T_752e3_row5_col3, #T_752e3_row5_col4, #T_752e3_row6_col2, #T_752e3_row6_col3, #T_752e3_row6_col4, #T_752e3_row7_col4, #T_752e3_row8_col4, #T_752e3_row9_col4, #T_752e3_row10_col4 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 100%, #1e1e1e 100%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row0_col5, #T_752e3_row0_col6, #T_752e3_row0_col7, #T_752e3_row1_col5, #T_752e3_row1_col6, #T_752e3_row1_col7, #T_752e3_row2_col5, #T_752e3_row2_col6, #T_752e3_row2_col7, #T_752e3_row3_col5, #T_752e3_row3_col6, #T_752e3_row3_col7, #T_752e3_row4_col5, #T_752e3_row4_col6, #T_752e3_row4_col7, #T_752e3_row5_col5, #T_752e3_row5_col6, #T_752e3_row5_col7, #T_752e3_row6_col5, #T_752e3_row6_col6, #T_752e3_row6_col7, #T_752e3_row7_col5, #T_752e3_row7_col6, #T_752e3_row7_col7, #T_752e3_row8_col5, #T_752e3_row8_col6, #T_752e3_row8_col7, #T_752e3_row9_col5, #T_752e3_row9_col6, #T_752e3_row9_col7, #T_752e3_row10_col5, #T_752e3_row10_col6, #T_752e3_row10_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 0%, #1e1e1e 0%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row1_col1, #T_752e3_row2_col1, #T_752e3_row3_col1, #T_752e3_row4_col1, #T_752e3_row5_col1, #T_752e3_row7_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 99%, #1e1e1e 99%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row6_col1, #T_752e3_row7_col2, #T_752e3_row8_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 98%, #1e1e1e 98%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row7_col1, #T_752e3_row9_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 91%, #1e1e1e 91%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row8_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 82%, #1e1e1e 82%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row8_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 97%, #1e1e1e 97%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row9_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 74%, #1e1e1e 74%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row9_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 96%, #1e1e1e 96%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row10_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 67%, #1e1e1e 67%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row10_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 90%, #1e1e1e 90%);\n",
       "  color: white;\n",
       "}\n",
       "#T_752e3_row10_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 95%, #1e1e1e 95%);\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_752e3\">\n",
       "  <caption>Benchmark CONCEPT (tolérant) - même cluster_id (def ou context)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_752e3_level0_col0\" class=\"col_heading level0 col0\" >Modèle</th>\n",
       "      <th id=\"T_752e3_level0_col1\" class=\"col_heading level0 col1\" >R@1 (%)</th>\n",
       "      <th id=\"T_752e3_level0_col2\" class=\"col_heading level0 col2\" >R@5 (%)</th>\n",
       "      <th id=\"T_752e3_level0_col3\" class=\"col_heading level0 col3\" >R@10 (%)</th>\n",
       "      <th id=\"T_752e3_level0_col4\" class=\"col_heading level0 col4\" >R@20 (%)</th>\n",
       "      <th id=\"T_752e3_level0_col5\" class=\"col_heading level0 col5\" >MRR@10</th>\n",
       "      <th id=\"T_752e3_level0_col6\" class=\"col_heading level0 col6\" >nDCG@10</th>\n",
       "      <th id=\"T_752e3_level0_col7\" class=\"col_heading level0 col7\" >TotalCompute(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row0_col0\" class=\"data row0 col0\" >E5-Large-instruct</td>\n",
       "      <td id=\"T_752e3_row0_col1\" class=\"data row0 col1\" >100.0</td>\n",
       "      <td id=\"T_752e3_row0_col2\" class=\"data row0 col2\" >100.0</td>\n",
       "      <td id=\"T_752e3_row0_col3\" class=\"data row0 col3\" >100.0</td>\n",
       "      <td id=\"T_752e3_row0_col4\" class=\"data row0 col4\" >100.0</td>\n",
       "      <td id=\"T_752e3_row0_col5\" class=\"data row0 col5\" >1.000</td>\n",
       "      <td id=\"T_752e3_row0_col6\" class=\"data row0 col6\" >0.989</td>\n",
       "      <td id=\"T_752e3_row0_col7\" class=\"data row0 col7\" >0.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row1_col0\" class=\"data row1 col0\" >E5-Large</td>\n",
       "      <td id=\"T_752e3_row1_col1\" class=\"data row1 col1\" >99.9</td>\n",
       "      <td id=\"T_752e3_row1_col2\" class=\"data row1 col2\" >100.0</td>\n",
       "      <td id=\"T_752e3_row1_col3\" class=\"data row1 col3\" >100.0</td>\n",
       "      <td id=\"T_752e3_row1_col4\" class=\"data row1 col4\" >100.0</td>\n",
       "      <td id=\"T_752e3_row1_col5\" class=\"data row1 col5\" >0.999</td>\n",
       "      <td id=\"T_752e3_row1_col6\" class=\"data row1 col6\" >0.993</td>\n",
       "      <td id=\"T_752e3_row1_col7\" class=\"data row1 col7\" >0.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row2_col0\" class=\"data row2 col0\" >E5-Large-FT-Config3</td>\n",
       "      <td id=\"T_752e3_row2_col1\" class=\"data row2 col1\" >99.9</td>\n",
       "      <td id=\"T_752e3_row2_col2\" class=\"data row2 col2\" >100.0</td>\n",
       "      <td id=\"T_752e3_row2_col3\" class=\"data row2 col3\" >100.0</td>\n",
       "      <td id=\"T_752e3_row2_col4\" class=\"data row2 col4\" >100.0</td>\n",
       "      <td id=\"T_752e3_row2_col5\" class=\"data row2 col5\" >0.999</td>\n",
       "      <td id=\"T_752e3_row2_col6\" class=\"data row2 col6\" >0.987</td>\n",
       "      <td id=\"T_752e3_row2_col7\" class=\"data row2 col7\" >0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row3_col0\" class=\"data row3 col0\" >E5-Large-FT-Config1</td>\n",
       "      <td id=\"T_752e3_row3_col1\" class=\"data row3 col1\" >99.6</td>\n",
       "      <td id=\"T_752e3_row3_col2\" class=\"data row3 col2\" >100.0</td>\n",
       "      <td id=\"T_752e3_row3_col3\" class=\"data row3 col3\" >100.0</td>\n",
       "      <td id=\"T_752e3_row3_col4\" class=\"data row3 col4\" >100.0</td>\n",
       "      <td id=\"T_752e3_row3_col5\" class=\"data row3 col5\" >0.998</td>\n",
       "      <td id=\"T_752e3_row3_col6\" class=\"data row3 col6\" >0.975</td>\n",
       "      <td id=\"T_752e3_row3_col7\" class=\"data row3 col7\" >0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row4_col0\" class=\"data row4 col0\" >E5-Large-FT-Config4</td>\n",
       "      <td id=\"T_752e3_row4_col1\" class=\"data row4 col1\" >99.4</td>\n",
       "      <td id=\"T_752e3_row4_col2\" class=\"data row4 col2\" >100.0</td>\n",
       "      <td id=\"T_752e3_row4_col3\" class=\"data row4 col3\" >100.0</td>\n",
       "      <td id=\"T_752e3_row4_col4\" class=\"data row4 col4\" >100.0</td>\n",
       "      <td id=\"T_752e3_row4_col5\" class=\"data row4 col5\" >0.996</td>\n",
       "      <td id=\"T_752e3_row4_col6\" class=\"data row4 col6\" >0.978</td>\n",
       "      <td id=\"T_752e3_row4_col7\" class=\"data row4 col7\" >0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row5_col0\" class=\"data row5 col0\" >Solon-Large</td>\n",
       "      <td id=\"T_752e3_row5_col1\" class=\"data row5 col1\" >99.1</td>\n",
       "      <td id=\"T_752e3_row5_col2\" class=\"data row5 col2\" >100.0</td>\n",
       "      <td id=\"T_752e3_row5_col3\" class=\"data row5 col3\" >100.0</td>\n",
       "      <td id=\"T_752e3_row5_col4\" class=\"data row5 col4\" >100.0</td>\n",
       "      <td id=\"T_752e3_row5_col5\" class=\"data row5 col5\" >0.994</td>\n",
       "      <td id=\"T_752e3_row5_col6\" class=\"data row5 col6\" >0.981</td>\n",
       "      <td id=\"T_752e3_row5_col7\" class=\"data row5 col7\" >0.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row6_col0\" class=\"data row6 col0\" >E5-Large-FT-Config2</td>\n",
       "      <td id=\"T_752e3_row6_col1\" class=\"data row6 col1\" >98.3</td>\n",
       "      <td id=\"T_752e3_row6_col2\" class=\"data row6 col2\" >100.0</td>\n",
       "      <td id=\"T_752e3_row6_col3\" class=\"data row6 col3\" >100.0</td>\n",
       "      <td id=\"T_752e3_row6_col4\" class=\"data row6 col4\" >100.0</td>\n",
       "      <td id=\"T_752e3_row6_col5\" class=\"data row6 col5\" >0.990</td>\n",
       "      <td id=\"T_752e3_row6_col6\" class=\"data row6 col6\" >0.966</td>\n",
       "      <td id=\"T_752e3_row6_col7\" class=\"data row6 col7\" >0.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row7_col0\" class=\"data row7 col0\" >Solon-Large-FT-Config2</td>\n",
       "      <td id=\"T_752e3_row7_col1\" class=\"data row7 col1\" >90.5</td>\n",
       "      <td id=\"T_752e3_row7_col2\" class=\"data row7 col2\" >97.9</td>\n",
       "      <td id=\"T_752e3_row7_col3\" class=\"data row7 col3\" >98.7</td>\n",
       "      <td id=\"T_752e3_row7_col4\" class=\"data row7 col4\" >99.4</td>\n",
       "      <td id=\"T_752e3_row7_col5\" class=\"data row7 col5\" >0.934</td>\n",
       "      <td id=\"T_752e3_row7_col6\" class=\"data row7 col6\" >0.890</td>\n",
       "      <td id=\"T_752e3_row7_col7\" class=\"data row7 col7\" >0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row8_col0\" class=\"data row8 col0\" >Solon-Large-FT-Config1</td>\n",
       "      <td id=\"T_752e3_row8_col1\" class=\"data row8 col1\" >81.9</td>\n",
       "      <td id=\"T_752e3_row8_col2\" class=\"data row8 col2\" >96.4</td>\n",
       "      <td id=\"T_752e3_row8_col3\" class=\"data row8 col3\" >97.2</td>\n",
       "      <td id=\"T_752e3_row8_col4\" class=\"data row8 col4\" >99.1</td>\n",
       "      <td id=\"T_752e3_row8_col5\" class=\"data row8 col5\" >0.880</td>\n",
       "      <td id=\"T_752e3_row8_col6\" class=\"data row8 col6\" >0.822</td>\n",
       "      <td id=\"T_752e3_row8_col7\" class=\"data row8 col7\" >0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row9_col0\" class=\"data row9 col0\" >Solon-Large-FT-Config4</td>\n",
       "      <td id=\"T_752e3_row9_col1\" class=\"data row9 col1\" >72.8</td>\n",
       "      <td id=\"T_752e3_row9_col2\" class=\"data row9 col2\" >89.0</td>\n",
       "      <td id=\"T_752e3_row9_col3\" class=\"data row9 col3\" >94.6</td>\n",
       "      <td id=\"T_752e3_row9_col4\" class=\"data row9 col4\" >97.6</td>\n",
       "      <td id=\"T_752e3_row9_col5\" class=\"data row9 col5\" >0.798</td>\n",
       "      <td id=\"T_752e3_row9_col6\" class=\"data row9 col6\" >0.738</td>\n",
       "      <td id=\"T_752e3_row9_col7\" class=\"data row9 col7\" >0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_752e3_row10_col0\" class=\"data row10 col0\" >Solon-Large-FT-Config3</td>\n",
       "      <td id=\"T_752e3_row10_col1\" class=\"data row10 col1\" >65.1</td>\n",
       "      <td id=\"T_752e3_row10_col2\" class=\"data row10 col2\" >86.4</td>\n",
       "      <td id=\"T_752e3_row10_col3\" class=\"data row10 col3\" >91.3</td>\n",
       "      <td id=\"T_752e3_row10_col4\" class=\"data row10 col4\" >95.9</td>\n",
       "      <td id=\"T_752e3_row10_col5\" class=\"data row10 col5\" >0.742</td>\n",
       "      <td id=\"T_752e3_row10_col6\" class=\"data row10 col6\" >0.691</td>\n",
       "      <td id=\"T_752e3_row10_col7\" class=\"data row10 col7\" >0.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2dfc95d0190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f1d5b th {\n",
       "  background-color: #111111;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_f1d5b td {\n",
       "  border-color: #333333;\n",
       "}\n",
       "#T_f1d5b caption {\n",
       "  caption-side: top;\n",
       "  color: white;\n",
       "  font-size: 14pt;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_f1d5b_row0_col0, #T_f1d5b_row1_col0, #T_f1d5b_row2_col0, #T_f1d5b_row3_col0, #T_f1d5b_row4_col0, #T_f1d5b_row5_col0, #T_f1d5b_row6_col0, #T_f1d5b_row7_col0, #T_f1d5b_row8_col0, #T_f1d5b_row9_col0, #T_f1d5b_row10_col0 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background-color: #1f3c88;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_f1d5b_row0_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 91%, #1e1e1e 91%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row0_col2, #T_f1d5b_row0_col3, #T_f1d5b_row1_col2, #T_f1d5b_row1_col3, #T_f1d5b_row2_col2, #T_f1d5b_row3_col3, #T_f1d5b_row4_col3, #T_f1d5b_row6_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 99%, #1e1e1e 99%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row0_col4, #T_f1d5b_row1_col4, #T_f1d5b_row2_col3, #T_f1d5b_row2_col4, #T_f1d5b_row3_col4, #T_f1d5b_row4_col4, #T_f1d5b_row5_col4, #T_f1d5b_row6_col4, #T_f1d5b_row7_col4, #T_f1d5b_row8_col4, #T_f1d5b_row9_col4, #T_f1d5b_row10_col4 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 100%, #1e1e1e 100%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row0_col5, #T_f1d5b_row0_col6, #T_f1d5b_row0_col7, #T_f1d5b_row1_col5, #T_f1d5b_row1_col6, #T_f1d5b_row1_col7, #T_f1d5b_row2_col5, #T_f1d5b_row2_col6, #T_f1d5b_row2_col7, #T_f1d5b_row3_col5, #T_f1d5b_row3_col6, #T_f1d5b_row3_col7, #T_f1d5b_row4_col5, #T_f1d5b_row4_col6, #T_f1d5b_row4_col7, #T_f1d5b_row5_col5, #T_f1d5b_row5_col6, #T_f1d5b_row5_col7, #T_f1d5b_row6_col5, #T_f1d5b_row6_col6, #T_f1d5b_row6_col7, #T_f1d5b_row7_col5, #T_f1d5b_row7_col6, #T_f1d5b_row7_col7, #T_f1d5b_row8_col5, #T_f1d5b_row8_col6, #T_f1d5b_row8_col7, #T_f1d5b_row9_col5, #T_f1d5b_row9_col6, #T_f1d5b_row9_col7, #T_f1d5b_row10_col5, #T_f1d5b_row10_col6, #T_f1d5b_row10_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 0%, #1e1e1e 0%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row1_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 89%, #1e1e1e 89%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row2_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 88%, #1e1e1e 88%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row3_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 80%, #1e1e1e 80%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row3_col2, #T_f1d5b_row4_col2, #T_f1d5b_row5_col2, #T_f1d5b_row5_col3, #T_f1d5b_row6_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 98%, #1e1e1e 98%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row4_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 78%, #1e1e1e 78%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row5_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 77%, #1e1e1e 77%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row6_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 75%, #1e1e1e 75%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row7_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 52%, #1e1e1e 52%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row7_col2, #T_f1d5b_row8_col2, #T_f1d5b_row9_col3, #T_f1d5b_row10_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 93%, #1e1e1e 93%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row7_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 96%, #1e1e1e 96%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row8_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 53%, #1e1e1e 53%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row8_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 97%, #1e1e1e 97%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row9_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 46%, #1e1e1e 46%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row9_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 87%, #1e1e1e 87%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row10_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 41%, #1e1e1e 41%);\n",
       "  color: white;\n",
       "}\n",
       "#T_f1d5b_row10_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 84%, #1e1e1e 84%);\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f1d5b\">\n",
       "  <caption>Benchmark DEF-ONLY (strict) - def du même cluster_id uniquement</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_f1d5b_level0_col0\" class=\"col_heading level0 col0\" >Modèle</th>\n",
       "      <th id=\"T_f1d5b_level0_col1\" class=\"col_heading level0 col1\" >R@1 (%)</th>\n",
       "      <th id=\"T_f1d5b_level0_col2\" class=\"col_heading level0 col2\" >R@5 (%)</th>\n",
       "      <th id=\"T_f1d5b_level0_col3\" class=\"col_heading level0 col3\" >R@10 (%)</th>\n",
       "      <th id=\"T_f1d5b_level0_col4\" class=\"col_heading level0 col4\" >R@20 (%)</th>\n",
       "      <th id=\"T_f1d5b_level0_col5\" class=\"col_heading level0 col5\" >MRR@10</th>\n",
       "      <th id=\"T_f1d5b_level0_col6\" class=\"col_heading level0 col6\" >nDCG@10</th>\n",
       "      <th id=\"T_f1d5b_level0_col7\" class=\"col_heading level0 col7\" >TotalCompute(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row0_col0\" class=\"data row0 col0\" >E5-Large-instruct</td>\n",
       "      <td id=\"T_f1d5b_row0_col1\" class=\"data row0 col1\" >91.7</td>\n",
       "      <td id=\"T_f1d5b_row0_col2\" class=\"data row0 col2\" >99.2</td>\n",
       "      <td id=\"T_f1d5b_row0_col3\" class=\"data row0 col3\" >99.9</td>\n",
       "      <td id=\"T_f1d5b_row0_col4\" class=\"data row0 col4\" >100.0</td>\n",
       "      <td id=\"T_f1d5b_row0_col5\" class=\"data row0 col5\" >0.955</td>\n",
       "      <td id=\"T_f1d5b_row0_col6\" class=\"data row0 col6\" >0.966</td>\n",
       "      <td id=\"T_f1d5b_row0_col7\" class=\"data row0 col7\" >0.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row1_col0\" class=\"data row1 col0\" >E5-Large-FT-Config3</td>\n",
       "      <td id=\"T_f1d5b_row1_col1\" class=\"data row1 col1\" >89.5</td>\n",
       "      <td id=\"T_f1d5b_row1_col2\" class=\"data row1 col2\" >99.2</td>\n",
       "      <td id=\"T_f1d5b_row1_col3\" class=\"data row1 col3\" >99.4</td>\n",
       "      <td id=\"T_f1d5b_row1_col4\" class=\"data row1 col4\" >99.9</td>\n",
       "      <td id=\"T_f1d5b_row1_col5\" class=\"data row1 col5\" >0.942</td>\n",
       "      <td id=\"T_f1d5b_row1_col6\" class=\"data row1 col6\" >0.956</td>\n",
       "      <td id=\"T_f1d5b_row1_col7\" class=\"data row1 col7\" >0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row2_col0\" class=\"data row2 col0\" >E5-Large-FT-Config1</td>\n",
       "      <td id=\"T_f1d5b_row2_col1\" class=\"data row2 col1\" >88.5</td>\n",
       "      <td id=\"T_f1d5b_row2_col2\" class=\"data row2 col2\" >99.0</td>\n",
       "      <td id=\"T_f1d5b_row2_col3\" class=\"data row2 col3\" >99.4</td>\n",
       "      <td id=\"T_f1d5b_row2_col4\" class=\"data row2 col4\" >99.4</td>\n",
       "      <td id=\"T_f1d5b_row2_col5\" class=\"data row2 col5\" >0.936</td>\n",
       "      <td id=\"T_f1d5b_row2_col6\" class=\"data row2 col6\" >0.951</td>\n",
       "      <td id=\"T_f1d5b_row2_col7\" class=\"data row2 col7\" >0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row3_col0\" class=\"data row3 col0\" >E5-Large</td>\n",
       "      <td id=\"T_f1d5b_row3_col1\" class=\"data row3 col1\" >80.0</td>\n",
       "      <td id=\"T_f1d5b_row3_col2\" class=\"data row3 col2\" >98.5</td>\n",
       "      <td id=\"T_f1d5b_row3_col3\" class=\"data row3 col3\" >99.1</td>\n",
       "      <td id=\"T_f1d5b_row3_col4\" class=\"data row3 col4\" >99.6</td>\n",
       "      <td id=\"T_f1d5b_row3_col5\" class=\"data row3 col5\" >0.891</td>\n",
       "      <td id=\"T_f1d5b_row3_col6\" class=\"data row3 col6\" >0.917</td>\n",
       "      <td id=\"T_f1d5b_row3_col7\" class=\"data row3 col7\" >0.864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row4_col0\" class=\"data row4 col0\" >Solon-Large</td>\n",
       "      <td id=\"T_f1d5b_row4_col1\" class=\"data row4 col1\" >78.1</td>\n",
       "      <td id=\"T_f1d5b_row4_col2\" class=\"data row4 col2\" >98.5</td>\n",
       "      <td id=\"T_f1d5b_row4_col3\" class=\"data row4 col3\" >98.8</td>\n",
       "      <td id=\"T_f1d5b_row4_col4\" class=\"data row4 col4\" >99.6</td>\n",
       "      <td id=\"T_f1d5b_row4_col5\" class=\"data row4 col5\" >0.880</td>\n",
       "      <td id=\"T_f1d5b_row4_col6\" class=\"data row4 col6\" >0.908</td>\n",
       "      <td id=\"T_f1d5b_row4_col7\" class=\"data row4 col7\" >0.813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row5_col0\" class=\"data row5 col0\" >E5-Large-FT-Config4</td>\n",
       "      <td id=\"T_f1d5b_row5_col1\" class=\"data row5 col1\" >76.9</td>\n",
       "      <td id=\"T_f1d5b_row5_col2\" class=\"data row5 col2\" >97.3</td>\n",
       "      <td id=\"T_f1d5b_row5_col3\" class=\"data row5 col3\" >97.8</td>\n",
       "      <td id=\"T_f1d5b_row5_col4\" class=\"data row5 col4\" >99.0</td>\n",
       "      <td id=\"T_f1d5b_row5_col5\" class=\"data row5 col5\" >0.869</td>\n",
       "      <td id=\"T_f1d5b_row5_col6\" class=\"data row5 col6\" >0.897</td>\n",
       "      <td id=\"T_f1d5b_row5_col7\" class=\"data row5 col7\" >0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row6_col0\" class=\"data row6 col0\" >E5-Large-FT-Config2</td>\n",
       "      <td id=\"T_f1d5b_row6_col1\" class=\"data row6 col1\" >75.3</td>\n",
       "      <td id=\"T_f1d5b_row6_col2\" class=\"data row6 col2\" >97.7</td>\n",
       "      <td id=\"T_f1d5b_row6_col3\" class=\"data row6 col3\" >98.2</td>\n",
       "      <td id=\"T_f1d5b_row6_col4\" class=\"data row6 col4\" >99.1</td>\n",
       "      <td id=\"T_f1d5b_row6_col5\" class=\"data row6 col5\" >0.862</td>\n",
       "      <td id=\"T_f1d5b_row6_col6\" class=\"data row6 col6\" >0.893</td>\n",
       "      <td id=\"T_f1d5b_row6_col7\" class=\"data row6 col7\" >0.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row7_col0\" class=\"data row7 col0\" >Solon-Large-FT-Config2</td>\n",
       "      <td id=\"T_f1d5b_row7_col1\" class=\"data row7 col1\" >51.0</td>\n",
       "      <td id=\"T_f1d5b_row7_col2\" class=\"data row7 col2\" >90.5</td>\n",
       "      <td id=\"T_f1d5b_row7_col3\" class=\"data row7 col3\" >93.8</td>\n",
       "      <td id=\"T_f1d5b_row7_col4\" class=\"data row7 col4\" >96.8</td>\n",
       "      <td id=\"T_f1d5b_row7_col5\" class=\"data row7 col5\" >0.688</td>\n",
       "      <td id=\"T_f1d5b_row7_col6\" class=\"data row7 col6\" >0.751</td>\n",
       "      <td id=\"T_f1d5b_row7_col7\" class=\"data row7 col7\" >0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row8_col0\" class=\"data row8 col0\" >Solon-Large-FT-Config1</td>\n",
       "      <td id=\"T_f1d5b_row8_col1\" class=\"data row8 col1\" >50.9</td>\n",
       "      <td id=\"T_f1d5b_row8_col2\" class=\"data row8 col2\" >88.2</td>\n",
       "      <td id=\"T_f1d5b_row8_col3\" class=\"data row8 col3\" >91.9</td>\n",
       "      <td id=\"T_f1d5b_row8_col4\" class=\"data row8 col4\" >94.0</td>\n",
       "      <td id=\"T_f1d5b_row8_col5\" class=\"data row8 col5\" >0.672</td>\n",
       "      <td id=\"T_f1d5b_row8_col6\" class=\"data row8 col6\" >0.734</td>\n",
       "      <td id=\"T_f1d5b_row8_col7\" class=\"data row8 col7\" >0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row9_col0\" class=\"data row9 col0\" >Solon-Large-FT-Config3</td>\n",
       "      <td id=\"T_f1d5b_row9_col1\" class=\"data row9 col1\" >40.1</td>\n",
       "      <td id=\"T_f1d5b_row9_col2\" class=\"data row9 col2\" >75.4</td>\n",
       "      <td id=\"T_f1d5b_row9_col3\" class=\"data row9 col3\" >81.0</td>\n",
       "      <td id=\"T_f1d5b_row9_col4\" class=\"data row9 col4\" >86.3</td>\n",
       "      <td id=\"T_f1d5b_row9_col5\" class=\"data row9 col5\" >0.558</td>\n",
       "      <td id=\"T_f1d5b_row9_col6\" class=\"data row9 col6\" >0.620</td>\n",
       "      <td id=\"T_f1d5b_row9_col7\" class=\"data row9 col7\" >0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_f1d5b_row10_col0\" class=\"data row10 col0\" >Solon-Large-FT-Config4</td>\n",
       "      <td id=\"T_f1d5b_row10_col1\" class=\"data row10 col1\" >37.6</td>\n",
       "      <td id=\"T_f1d5b_row10_col2\" class=\"data row10 col2\" >76.4</td>\n",
       "      <td id=\"T_f1d5b_row10_col3\" class=\"data row10 col3\" >84.2</td>\n",
       "      <td id=\"T_f1d5b_row10_col4\" class=\"data row10 col4\" >90.1</td>\n",
       "      <td id=\"T_f1d5b_row10_col5\" class=\"data row10 col5\" >0.551</td>\n",
       "      <td id=\"T_f1d5b_row10_col6\" class=\"data row10 col6\" >0.623</td>\n",
       "      <td id=\"T_f1d5b_row10_col7\" class=\"data row10 col7\" >0.799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2dfc95d5790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONFIG\n",
    "INPUT_JSONL = Path(\"bercy_test_10.jsonl\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "K_EVAL = 20\n",
    "MRR_K = 10\n",
    "NDCG_K = 10\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "BLOCK_SIZE = 128 if DEVICE == \"cuda\" else 32\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "LOCAL_DIR = Path(\"./final_models\")\n",
    "\n",
    "TASK = \"Retrieve the definition or context of an administrative acronym or term.\"\n",
    "INSTRUCTION_PREFIX = f\"Instruct: {TASK}\\nQuery: \"\n",
    "PASSAGE_PREFIX = \"passage: \"\n",
    "\n",
    "# MODELS (Modèles d'embeddings)\n",
    "CANDIDATS = [\n",
    "    {\"name\": \"Solon-Large\", \"id\": \"OrdalieTech/SOLON-embeddings-large-0.1\"},\n",
    "    {\"name\": \"Solon-Large-FT-Config1\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config1_merged\")},\n",
    "    {\"name\": \"Solon-Large-FT-Config2\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config2_merged\")},\n",
    "    {\"name\": \"Solon-Large-FT-Config3\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config3_merged\")},\n",
    "    {\"name\": \"Solon-Large-FT-Config4\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config4_merged\")},\n",
    "    {\"name\": \"E5-Large-instruct\", \"id\": \"intfloat/multilingual-e5-large-instruct\"},\n",
    "    {\"name\": \"E5-Large-FT-Config1\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config1_merged\")},\n",
    "    {\"name\": \"E5-Large-FT-Config2\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config2_merged\")},\n",
    "    {\"name\": \"E5-Large-FT-Config3\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config3_merged\")},\n",
    "    {\"name\": \"E5-Large-FT-Config4\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config4_merged\")},\n",
    "]\n",
    "\n",
    "def model_prefixes(model_name: str) -> Tuple[str, str]:\n",
    "    # 1) instruct + fine-tuned instruct\n",
    "    if (\"E5-Large-instruct\" in model_name) or (\"E5-Large-FT\" in model_name):\n",
    "        return INSTRUCTION_PREFIX, \"\"   # docs sans prefix pour instruct\n",
    "\n",
    "    # 2) E5 non-instruc\n",
    "    if model_name == \"E5-Large\":\n",
    "        return \"query: \", \"passage: \"\n",
    "\n",
    "    return \"\", \"\"\n",
    "\n",
    "def norm_space(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().split())\n",
    "\n",
    "def cuda_empty_cache():\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class SnowflakeWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper pour utiliser Snowflake (HF transformers) avec une API encode() proche SentenceTransformer.\n",
    "    Mean pooling + normalisation L2.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        if hasattr(config, \"use_memory_efficient_attention\"):\n",
    "            config.use_memory_efficient_attention = False\n",
    "        if hasattr(config, \"unpad_inputs\"):\n",
    "            config.unpad_inputs = False\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            add_pooling_layer=False,\n",
    "            trust_remote_code=True\n",
    "        ).to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        return summed / denom\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        prompt_name: Optional[str] = None,\n",
    "        batch_size: int = 32,\n",
    "        prefix_query: str = \"query: \",\n",
    "        prefix_doc: str = \"\"\n",
    "    ) -> torch.Tensor:\n",
    "        prefix = prefix_query if prompt_name == \"query\" else prefix_doc\n",
    "        inputs = [prefix + s for s in sentences]\n",
    "\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_texts = inputs[i:i + batch_size]\n",
    "            batch_tokens = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=MAX_LENGTH\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model(**batch_tokens)\n",
    "                emb = self._mean_pooling(outputs[0], batch_tokens[\"attention_mask\"])\n",
    "                emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "                all_embeddings.append(emb)\n",
    "\n",
    "        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty((0, 0), device=DEVICE)\n",
    "\n",
    "# 1) LOAD JSONL\n",
    "rows = []\n",
    "with INPUT_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "print(f\"Loaded rows: {len(rows)}\")\n",
    "\n",
    "# 2) BUILD DOCS + QUERIES\n",
    "DOCS: List[str] = []\n",
    "DOC_META: List[dict] = []\n",
    "DOC_INDEX_BY_CLUSTER_AND_TYPE: Dict[Tuple[str, str], int] = {}  # (cluster_id, pair_type) -> doc_idx\n",
    "DOC_INDICES_BY_CLUSTER: Dict[str, List[int]] = defaultdict(list)\n",
    "\n",
    "QUERIES: List[str] = []\n",
    "Q_META: List[dict] = []\n",
    "\n",
    "# Docs: one doc for each (cluster_id, def) and (cluster_id, context)\n",
    "for r in rows:\n",
    "    cid = r.get(\"cluster_id\")\n",
    "    ptype = r.get(\"pair_type\")\n",
    "    pos = norm_space(r.get(\"positive\", \"\"))\n",
    "    if not cid or ptype not in {\"def\", \"context\"} or not pos:\n",
    "        continue\n",
    "\n",
    "    key = (cid, ptype)\n",
    "    if key in DOC_INDEX_BY_CLUSTER_AND_TYPE:\n",
    "        continue\n",
    "\n",
    "    doc_idx = len(DOCS)\n",
    "    DOCS.append(pos)\n",
    "    DOC_META.append({\"cluster_id\": cid, \"pair_type\": ptype})\n",
    "    DOC_INDEX_BY_CLUSTER_AND_TYPE[key] = doc_idx\n",
    "    DOC_INDICES_BY_CLUSTER[cid].append(doc_idx)\n",
    "\n",
    "print(f\"Docs built (def+context): {len(DOCS)}\")\n",
    "\n",
    "# Queries:\n",
    "# - from def/context: anchor keyword + \"C'est quoi\"\n",
    "# - from qa: anchor (question)\n",
    "for r in rows:\n",
    "    cid = r.get(\"cluster_id\")\n",
    "    ptype = r.get(\"pair_type\")\n",
    "    anchor = norm_space(r.get(\"anchor\", \"\"))\n",
    "\n",
    "    if not cid or not anchor:\n",
    "        continue\n",
    "\n",
    "    if ptype in {\"def\", \"context\"}:\n",
    "        QUERIES.append(anchor)\n",
    "        Q_META.append({\"cluster_id\": cid, \"kind\": \"anchor_keyword\", \"source_pair_type\": ptype})\n",
    "\n",
    "        QUERIES.append(f\"C'est quoi {anchor} ?\")\n",
    "        Q_META.append({\"cluster_id\": cid, \"kind\": \"anchor_cestquoi\", \"source_pair_type\": ptype})\n",
    "\n",
    "    elif ptype == \"qa\":\n",
    "        QUERIES.append(anchor)\n",
    "        Q_META.append({\"cluster_id\": cid, \"kind\": \"qa_question\", \"source_pair_type\": \"qa\"})\n",
    "\n",
    "print(f\"Queries built: {len(QUERIES)}\")\n",
    "\n",
    "# 3) GROUND TRUTH (2 versions)\n",
    "GT_CONCEPT: List[Set[int]] = []\n",
    "GT_DEFONLY: List[Set[int]] = []\n",
    "\n",
    "missing_concept = 0\n",
    "missing_def = 0\n",
    "\n",
    "for qm in Q_META:\n",
    "    cid = qm[\"cluster_id\"]\n",
    "\n",
    "    rel_concept = set(DOC_INDICES_BY_CLUSTER.get(cid, []))  # def + context\n",
    "    if not rel_concept:\n",
    "        missing_concept += 1\n",
    "    GT_CONCEPT.append(rel_concept)\n",
    "\n",
    "    def_idx = DOC_INDEX_BY_CLUSTER_AND_TYPE.get((cid, \"def\"))\n",
    "    if def_idx is None:\n",
    "        missing_def += 1\n",
    "        GT_DEFONLY.append(set())\n",
    "    else:\n",
    "        GT_DEFONLY.append({def_idx})\n",
    "\n",
    "print(f\"Missing concept GT: {missing_concept}\")\n",
    "print(f\"Missing def-only GT: {missing_def}\")\n",
    "\n",
    "# 4) RETRIEVAL TOPK (blockwise)\n",
    "def compute_topk_blockwise(emb_q: torch.Tensor, emb_d: torch.Tensor, k_eval: int, block_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    emb_q: [Nq, dim], emb_d: [Nd, dim]\n",
    "    returns indices: [Nq, K]\n",
    "    \"\"\"\n",
    "    Nq = emb_q.size(0)\n",
    "    K = min(k_eval, emb_d.size(0))\n",
    "    emb_d_t = emb_d.T\n",
    "\n",
    "    out = []\n",
    "    for i in range(0, Nq, block_size):\n",
    "        q_block = emb_q[i:i + block_size]\n",
    "        scores = torch.matmul(q_block, emb_d_t)  # cosine if normalized\n",
    "        topk = torch.topk(scores, k=K, dim=1).indices\n",
    "        out.append(topk.detach().cpu())\n",
    "        del scores, topk, q_block\n",
    "        cuda_empty_cache()\n",
    "\n",
    "    return torch.cat(out, dim=0)\n",
    "\n",
    "def compute_metrics_from_topk(\n",
    "    topk_indices: torch.Tensor,\n",
    "    ground_truth: List[Set[int]],\n",
    "    mrr_k: int,\n",
    "    ndcg_k: int\n",
    ") -> Dict[str, float]:\n",
    "    Nq, K = topk_indices.shape\n",
    "    k5 = min(5, K)\n",
    "    k10 = min(10, K)\n",
    "    k20 = min(20, K)\n",
    "    mrr_k = min(mrr_k, K)\n",
    "    ndcg_k = min(ndcg_k, K)\n",
    "\n",
    "    r1 = r5 = r10 = r20 = 0\n",
    "    mrr = 0.0\n",
    "    ndcg = 0.0\n",
    "\n",
    "    for i in range(Nq):\n",
    "        relevant = ground_truth[i]\n",
    "        ranked = topk_indices[i].tolist()\n",
    "\n",
    "        if relevant:\n",
    "            if ranked[0] in relevant:\n",
    "                r1 += 1\n",
    "            if any(idx in relevant for idx in ranked[:k5]):\n",
    "                r5 += 1\n",
    "            if any(idx in relevant for idx in ranked[:k10]):\n",
    "                r10 += 1\n",
    "            if any(idx in relevant for idx in ranked[:k20]):\n",
    "                r20 += 1\n",
    "\n",
    "            rr = 0.0\n",
    "            for rank_pos, doc_idx in enumerate(ranked[:mrr_k], start=1):\n",
    "                if doc_idx in relevant:\n",
    "                    rr = 1.0 / rank_pos\n",
    "                    break\n",
    "            mrr += rr\n",
    "\n",
    "            dcg = 0.0\n",
    "            for rank_pos, doc_idx in enumerate(ranked[:ndcg_k], start=1):\n",
    "                if doc_idx in relevant:\n",
    "                    dcg += 1.0 / np.log2(rank_pos + 1)\n",
    "\n",
    "            rel_count = min(len(relevant), ndcg_k)\n",
    "            idcg = 0.0\n",
    "            for rank_pos in range(1, rel_count + 1):\n",
    "                idcg += 1.0 / np.log2(rank_pos + 1)\n",
    "\n",
    "            ndcg += (dcg / idcg) if idcg > 0 else 0.0\n",
    "        else:\n",
    "            # si GT vide pour cette query : on ignore (ou alors ça pénalise)\n",
    "            # ici on pénalise implicitement car rr=0, recalls=0, ndcg=0\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"N_queries\": Nq,\n",
    "        \"R@1 (%)\": (r1 / Nq) * 100,\n",
    "        \"R@5 (%)\": (r5 / Nq) * 100,\n",
    "        \"R@10 (%)\": (r10 / Nq) * 100,\n",
    "        \"R@20 (%)\": (r20 / Nq) * 100,\n",
    "        f\"MRR@{mrr_k}\": mrr / Nq,\n",
    "        f\"nDCG@{ndcg_k}\": ndcg / Nq,\n",
    "    }\n",
    "\n",
    "# 5) ENCODE HELPERS\n",
    "def encode_st(model: SentenceTransformer, texts: List[str], prefix: str) -> torch.Tensor:\n",
    "    emb = model.encode(\n",
    "        [prefix + t for t in texts],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,   # cosine = dot product\n",
    "    )\n",
    "    return emb\n",
    "\n",
    "\n",
    "# 6) LOAD & ENCODE MODEL HELPERS\n",
    "def load_model_smart(cand):\n",
    "    t0 = time.perf_counter()\n",
    "    model = None\n",
    "    err = None\n",
    "\n",
    "    try:\n",
    "        if \"Snowflake\" in cand[\"name\"]:\n",
    "            model = SnowflakeWrapper(cand[\"id\"])\n",
    "        else:\n",
    "            kw = {\"torch_dtype\": torch.float16} if DEVICE == \"cuda\" else {}\n",
    "            trust = cand.get(\"trust\", False)\n",
    "            model = SentenceTransformer(cand[\"id\"], trust_remote_code=trust, device=DEVICE, model_kwargs=kw)\n",
    "    except Exception as e:\n",
    "        err = e\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    return model, (t1 - t0), err\n",
    "\n",
    "\n",
    "def encode_any(model, model_name: str, texts: List[str], prefix: str, is_query: bool) -> torch.Tensor:\n",
    "    # Snowflake : wrapper transformers\n",
    "    if \"Snowflake\" in model_name:\n",
    "        return model.encode(\n",
    "            texts,\n",
    "            prompt_name=\"query\" if is_query else None,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            prefix_query=prefix,\n",
    "            prefix_doc=prefix\n",
    "        )\n",
    "    # SentenceTransformer\n",
    "    return model.encode(\n",
    "        [prefix + t for t in texts],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "\n",
    "# 7) RUN BENCHMARK\n",
    "results_concept = []\n",
    "results_defonly = []\n",
    "\n",
    "for cand in CANDIDATS:\n",
    "    gc.collect()\n",
    "    cuda_empty_cache()\n",
    "\n",
    "    name = cand[\"name\"]\n",
    "    mid = cand[\"id\"]\n",
    "    trust = cand.get(\"trust\", False)\n",
    "\n",
    "    print(f\"\\nLoading: {name} ({mid}) ...\")\n",
    "    model, load_s, err = load_model_smart(cand)\n",
    "    if err is not None or model is None:\n",
    "        print(f\"Load error for {name}: {err}\")\n",
    "        continue\n",
    "\n",
    "    pq, pd_ = model_prefixes(name)\n",
    "\n",
    "    # Encode queries\n",
    "    t0 = time.perf_counter()\n",
    "    emb_q = encode_any(model, name, QUERIES, pq, is_query=True)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Encode docs\n",
    "    emb_d = encode_any(model, name, DOCS, pd_, is_query=False)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    # Retrieval\n",
    "    t3 = time.perf_counter()\n",
    "    topk = compute_topk_blockwise(emb_q.to(DEVICE), emb_d.to(DEVICE), k_eval=K_EVAL, block_size=BLOCK_SIZE)\n",
    "    t4 = time.perf_counter()\n",
    "\n",
    "    # Metrics\n",
    "    m_concept = compute_metrics_from_topk(topk, GT_CONCEPT, mrr_k=MRR_K, ndcg_k=NDCG_K)\n",
    "    m_defonly = compute_metrics_from_topk(topk, GT_DEFONLY, mrr_k=MRR_K, ndcg_k=NDCG_K)\n",
    "\n",
    "    # Timings\n",
    "    encq_s = t1 - t0\n",
    "    encd_s = t2 - t1\n",
    "    retr_s = t4 - t3\n",
    "    total_s = t4 - t0\n",
    "\n",
    "    row_common = {\n",
    "        \"Modèle\": name,\n",
    "        \"Load(s)\": load_s,\n",
    "        \"EncQ(s)\": encq_s,\n",
    "        \"EncD(s)\": encd_s,\n",
    "        \"Retr(s)\": retr_s,\n",
    "        \"TotalCompute(s)\": total_s,\n",
    "        \"Nq\": m_concept[\"N_queries\"],\n",
    "        \"Nd\": len(DOCS),\n",
    "        \"K\": min(K_EVAL, len(DOCS)),\n",
    "    }\n",
    "\n",
    "    results_concept.append({**row_common, **{k: v for k, v in m_concept.items() if k != \"N_queries\"}})\n",
    "    results_defonly.append({**row_common, **{k: v for k, v in m_defonly.items() if k != \"N_queries\"}})\n",
    "\n",
    "    print(\n",
    "        f\"{name} | Concept R@1={results_concept[-1]['R@1 (%)']:.1f}% \"\n",
    "        f\"| DefOnly R@1={results_defonly[-1]['R@1 (%)']:.1f}% \"\n",
    "        f\"| Total={total_s:.1f}s\"\n",
    "    )\n",
    "\n",
    "    del model, emb_q, emb_d\n",
    "    gc.collect()\n",
    "    cuda_empty_cache()\n",
    "\n",
    "# 8) FONCTIONS DISPLAY\n",
    "def row_gradient(val, row_min, row_max):\n",
    "    if row_max == row_min:\n",
    "        pct = 0\n",
    "    else:\n",
    "        pct = (val - row_min) / (row_max - row_min)\n",
    "    pct = int(pct * 100)\n",
    "\n",
    "    return (\n",
    "        f\"background: linear-gradient(90deg, \"\n",
    "        f\"#1f3c88 {pct}%, \"\n",
    "        f\"#1e1e1e {pct}%);\"\n",
    "        f\"color: white;\"\n",
    "    )\n",
    "\n",
    "def apply_rowwise_gradient(df, cols):\n",
    "    styles = pd.DataFrame(\"\", index=df.index, columns=df.columns)\n",
    "    for idx in df.index:\n",
    "        row_vals = df.loc[idx, cols].astype(float)\n",
    "        rmin, rmax = row_vals.min(), row_vals.max()\n",
    "        for col in cols:\n",
    "            styles.loc[idx, col] = row_gradient(df.loc[idx, col], rmin, rmax)\n",
    "    return styles\n",
    "\n",
    "def display_ranked_table(df_sorted: pd.DataFrame, title: str):\n",
    "    # colonnes attendues\n",
    "    gradient_cols = [\n",
    "        \"R@1 (%)\",\n",
    "        \"R@5 (%)\",\n",
    "        \"R@10 (%)\",\n",
    "        \"R@20 (%)\",\n",
    "        f\"MRR@{MRR_K}\",\n",
    "        f\"nDCG@{NDCG_K}\",\n",
    "        \"TotalCompute(s)\",\n",
    "    ]\n",
    "    gradient_cols = [c for c in gradient_cols if c in df_sorted.columns]\n",
    "\n",
    "    df_display = df_sorted[[\"Modèle\"] + gradient_cols].copy().reset_index(drop=True)\n",
    "\n",
    "    fmt = {\n",
    "        \"R@1 (%)\": \"{:.1f}\",\n",
    "        \"R@5 (%)\": \"{:.1f}\",\n",
    "        \"R@10 (%)\": \"{:.1f}\",\n",
    "        \"R@20 (%)\": \"{:.1f}\",\n",
    "        f\"MRR@{MRR_K}\": \"{:.3f}\",\n",
    "        f\"nDCG@{NDCG_K}\": \"{:.3f}\",\n",
    "        \"TotalCompute(s)\": \"{:.3f}\",\n",
    "    }\n",
    "    fmt = {k: v for k, v in fmt.items() if k in df_display.columns}\n",
    "\n",
    "    styled = (\n",
    "        df_display.style\n",
    "        .format(fmt)\n",
    "        .hide(axis=\"index\")\n",
    "        .set_caption(title)\n",
    "        .set_properties(**{\n",
    "            \"background-color\": \"#1e1e1e\",\n",
    "            \"color\": \"white\",\n",
    "            \"border-color\": \"#333333\",\n",
    "            \"text-align\": \"center\",\n",
    "            \"font-size\": \"12pt\"\n",
    "        })\n",
    "        .set_table_styles([\n",
    "            {\"selector\": \"th\", \"props\": [\n",
    "                (\"background-color\", \"#111111\"),\n",
    "                (\"color\", \"white\"),\n",
    "                (\"border-color\", \"#333333\"),\n",
    "                (\"text-align\", \"center\")\n",
    "            ]},\n",
    "            {\"selector\": \"td\", \"props\": [\n",
    "                (\"border-color\", \"#333333\")\n",
    "            ]},\n",
    "            {\"selector\": \"caption\", \"props\": [\n",
    "                (\"caption-side\", \"top\"),\n",
    "                (\"color\", \"white\"),\n",
    "                (\"font-size\", \"14pt\"),\n",
    "                (\"font-weight\", \"bold\")\n",
    "            ]}\n",
    "        ])\n",
    "        .apply(lambda _: apply_rowwise_gradient(df_display, gradient_cols), axis=None)\n",
    "    )\n",
    "\n",
    "    # Bleu uniquement sur la colonne Modèle\n",
    "    styled = styled.set_properties(\n",
    "        subset=[\"Modèle\"],\n",
    "        **{\n",
    "            \"background-color\": \"#1f3c88\",\n",
    "            \"color\": \"white\",\n",
    "            \"font-weight\": \"bold\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display(styled)\n",
    "\n",
    "# 9) DISPLAY TABLES\n",
    "df_concept = pd.DataFrame(results_concept)\n",
    "df_defonly = pd.DataFrame(results_defonly)\n",
    "\n",
    "df_concept_sorted = df_concept.sort_values(by=\"R@1 (%)\", ascending=False).reset_index(drop=True)\n",
    "df_defonly_sorted = df_defonly.sort_values(by=\"R@1 (%)\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "display_ranked_table(df_concept_sorted, \"Benchmark CONCEPT (tolérant) - même cluster_id (def ou context)\")\n",
    "display_ranked_table(df_defonly_sorted, \"Benchmark DEF-ONLY (strict) - def du même cluster_id uniquement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da5d39",
   "metadata": {},
   "source": [
    "### **Evaluation sur le dataset complet (4500 lignes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ab6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = \"Retrieve the definition or context of an administrative acronym or term.\"\n",
    "INSTRUCT_Q = f\"Instruct: {TASK}\\nQuery: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d9a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 4500\n",
      "Docs built (def+context): 3000\n",
      "Queries built: 7500\n",
      "Missing concept GT: 0\n",
      "Missing def-only GT: 0\n",
      "\n",
      "Loading: Solon-Large (OrdalieTech/SOLON-embeddings-large-0.1) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afe97791bc0453ab28a628683264a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbc0c7d3354486e857c74d7df87c1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large | Concept R@1=95.7% | DefOnly R@1=76.2% | Total=9.3s\n",
      "\n",
      "Loading: Solon-Large-FT-Config1 (final_models\\solon_large_finetuned_config1_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config1_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb956095269470182be94bb60641247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d9ba4cb160430e8004969e57b4db1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config1 | Concept R@1=50.9% | DefOnly R@1=28.0% | Total=7.9s\n",
      "\n",
      "Loading: Solon-Large-FT-Config2 (final_models\\solon_large_finetuned_config2_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config2_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a055dbf3b343a2b137406684d0d5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f15d42dbbb94c07b1ad1b1dfff626a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config2 | Concept R@1=56.7% | DefOnly R@1=31.1% | Total=8.1s\n",
      "\n",
      "Loading: Solon-Large-FT-Config3 (final_models\\solon_large_finetuned_config3_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config3_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c157ac40dc734aaea9d944afde1b5111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0574eb610cc47c783bedb694e6d3430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config3 | Concept R@1=34.6% | DefOnly R@1=16.9% | Total=8.5s\n",
      "\n",
      "Loading: Solon-Large-FT-Config4 (final_models\\solon_large_finetuned_config4_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\solon_large_finetuned_config4_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16ffdf8f4654f40acccdc194005860c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85a6a8e5e4c449da50a833abfc0edb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solon-Large-FT-Config4 | Concept R@1=36.2% | DefOnly R@1=15.9% | Total=7.4s\n",
      "\n",
      "Loading: E5-Large (intfloat/multilingual-e5-large) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad20e8c44ef494897a57f113fda1ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c1c7f8c3384faf96e7ca2c405b0a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large | Concept R@1=98.7% | DefOnly R@1=80.8% | Total=9.3s\n",
      "\n",
      "Loading: E5-Large-instruct (intfloat/multilingual-e5-large-instruct) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff69df3a12a4b17ad012385b572b1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ea675b1507452490f9460a9fe39c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large-instruct | Concept R@1=97.6% | DefOnly R@1=91.3% | Total=7.6s\n",
      "\n",
      "Loading: E5-Large-FT-Config1 (final_models\\e5_large_finetuned_config1_merged) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from 'final_models\\e5_large_finetuned_config1_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f6e2c656294d43abbdaff09a115358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f992cf16d0a8461dad32d8184456b18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5-Large-FT-Config1 | Concept R@1=0.3% | DefOnly R@1=0.3% | Total=8.8s\n",
      "\n",
      "Loading: E5-Large-FT-Config2 (final_models\\e5_large_finetuned_config2_merged) ...\n",
      "Load error for E5-Large-FT-Config2: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory final_models\\e5_large_finetuned_config2_merged.\n",
      "\n",
      "Loading: E5-Large-FT-Config3 (final_models\\e5_large_finetuned_config3_merged) ...\n",
      "Load error for E5-Large-FT-Config3: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory final_models\\e5_large_finetuned_config3_merged.\n",
      "\n",
      "Loading: E5-Large-FT-Config4 (final_models\\e5_large_finetuned_config4_merged) ...\n",
      "Load error for E5-Large-FT-Config4: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory final_models\\e5_large_finetuned_config4_merged.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_12e38 th {\n",
       "  background-color: #111111;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_12e38 td {\n",
       "  border-color: #333333;\n",
       "}\n",
       "#T_12e38 caption {\n",
       "  caption-side: top;\n",
       "  color: white;\n",
       "  font-size: 14pt;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_12e38_row0_col0, #T_12e38_row1_col0, #T_12e38_row2_col0, #T_12e38_row3_col0, #T_12e38_row4_col0, #T_12e38_row5_col0, #T_12e38_row6_col0, #T_12e38_row7_col0 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background-color: #1f3c88;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_12e38_row0_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 98%, #1e1e1e 98%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row0_col2, #T_12e38_row0_col3, #T_12e38_row1_col2, #T_12e38_row1_col3, #T_12e38_row2_col2, #T_12e38_row2_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 99%, #1e1e1e 99%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row0_col4, #T_12e38_row1_col4, #T_12e38_row2_col4, #T_12e38_row3_col4, #T_12e38_row4_col4, #T_12e38_row5_col4, #T_12e38_row6_col4, #T_12e38_row7_col4 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 100%, #1e1e1e 100%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row0_col5, #T_12e38_row0_col6, #T_12e38_row1_col5, #T_12e38_row1_col6, #T_12e38_row2_col5, #T_12e38_row2_col6, #T_12e38_row3_col5, #T_12e38_row3_col6, #T_12e38_row4_col5, #T_12e38_row4_col6, #T_12e38_row5_col5, #T_12e38_row5_col6, #T_12e38_row6_col5, #T_12e38_row6_col6, #T_12e38_row7_col5, #T_12e38_row7_col6 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 0%, #1e1e1e 0%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row0_col7, #T_12e38_row2_col7, #T_12e38_row4_col7, #T_12e38_row5_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 8%, #1e1e1e 8%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row1_col1, #T_12e38_row3_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 97%, #1e1e1e 97%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row1_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 6%, #1e1e1e 6%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row2_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 95%, #1e1e1e 95%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row3_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 57%, #1e1e1e 57%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row3_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 91%, #1e1e1e 91%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row3_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 7%, #1e1e1e 7%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row4_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 56%, #1e1e1e 56%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row4_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 84%, #1e1e1e 84%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row4_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 93%, #1e1e1e 93%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row5_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 43%, #1e1e1e 43%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row5_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 80%, #1e1e1e 80%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row5_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 92%, #1e1e1e 92%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row6_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 45%, #1e1e1e 45%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row6_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 78%, #1e1e1e 78%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row6_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 90%, #1e1e1e 90%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row6_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 10%, #1e1e1e 10%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row7_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 2%, #1e1e1e 2%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row7_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 9%, #1e1e1e 9%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row7_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 37%, #1e1e1e 37%);\n",
       "  color: white;\n",
       "}\n",
       "#T_12e38_row7_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 74%, #1e1e1e 74%);\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_12e38\">\n",
       "  <caption>Benchmark CONCEPT (tolérant) - même cluster_id (def ou context)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_12e38_level0_col0\" class=\"col_heading level0 col0\" >Modèle</th>\n",
       "      <th id=\"T_12e38_level0_col1\" class=\"col_heading level0 col1\" >R@1 (%)</th>\n",
       "      <th id=\"T_12e38_level0_col2\" class=\"col_heading level0 col2\" >R@5 (%)</th>\n",
       "      <th id=\"T_12e38_level0_col3\" class=\"col_heading level0 col3\" >R@10 (%)</th>\n",
       "      <th id=\"T_12e38_level0_col4\" class=\"col_heading level0 col4\" >R@20 (%)</th>\n",
       "      <th id=\"T_12e38_level0_col5\" class=\"col_heading level0 col5\" >MRR@10</th>\n",
       "      <th id=\"T_12e38_level0_col6\" class=\"col_heading level0 col6\" >nDCG@10</th>\n",
       "      <th id=\"T_12e38_level0_col7\" class=\"col_heading level0 col7\" >TotalCompute(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row0_col0\" class=\"data row0 col0\" >E5-Large</td>\n",
       "      <td id=\"T_12e38_row0_col1\" class=\"data row0 col1\" >98.7</td>\n",
       "      <td id=\"T_12e38_row0_col2\" class=\"data row0 col2\" >99.8</td>\n",
       "      <td id=\"T_12e38_row0_col3\" class=\"data row0 col3\" >99.9</td>\n",
       "      <td id=\"T_12e38_row0_col4\" class=\"data row0 col4\" >100.0</td>\n",
       "      <td id=\"T_12e38_row0_col5\" class=\"data row0 col5\" >0.992</td>\n",
       "      <td id=\"T_12e38_row0_col6\" class=\"data row0 col6\" >0.970</td>\n",
       "      <td id=\"T_12e38_row0_col7\" class=\"data row0 col7\" >9.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row1_col0\" class=\"data row1 col0\" >E5-Large-instruct</td>\n",
       "      <td id=\"T_12e38_row1_col1\" class=\"data row1 col1\" >97.6</td>\n",
       "      <td id=\"T_12e38_row1_col2\" class=\"data row1 col2\" >99.6</td>\n",
       "      <td id=\"T_12e38_row1_col3\" class=\"data row1 col3\" >99.8</td>\n",
       "      <td id=\"T_12e38_row1_col4\" class=\"data row1 col4\" >99.8</td>\n",
       "      <td id=\"T_12e38_row1_col5\" class=\"data row1 col5\" >0.985</td>\n",
       "      <td id=\"T_12e38_row1_col6\" class=\"data row1 col6\" >0.848</td>\n",
       "      <td id=\"T_12e38_row1_col7\" class=\"data row1 col7\" >7.563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row2_col0\" class=\"data row2 col0\" >Solon-Large</td>\n",
       "      <td id=\"T_12e38_row2_col1\" class=\"data row2 col1\" >95.7</td>\n",
       "      <td id=\"T_12e38_row2_col2\" class=\"data row2 col2\" >99.1</td>\n",
       "      <td id=\"T_12e38_row2_col3\" class=\"data row2 col3\" >99.5</td>\n",
       "      <td id=\"T_12e38_row2_col4\" class=\"data row2 col4\" >99.7</td>\n",
       "      <td id=\"T_12e38_row2_col5\" class=\"data row2 col5\" >0.972</td>\n",
       "      <td id=\"T_12e38_row2_col6\" class=\"data row2 col6\" >0.921</td>\n",
       "      <td id=\"T_12e38_row2_col7\" class=\"data row2 col7\" >9.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row3_col0\" class=\"data row3 col0\" >Solon-Large-FT-Config2</td>\n",
       "      <td id=\"T_12e38_row3_col1\" class=\"data row3 col1\" >56.7</td>\n",
       "      <td id=\"T_12e38_row3_col2\" class=\"data row3 col2\" >89.3</td>\n",
       "      <td id=\"T_12e38_row3_col3\" class=\"data row3 col3\" >95.5</td>\n",
       "      <td id=\"T_12e38_row3_col4\" class=\"data row3 col4\" >97.5</td>\n",
       "      <td id=\"T_12e38_row3_col5\" class=\"data row3 col5\" >0.692</td>\n",
       "      <td id=\"T_12e38_row3_col6\" class=\"data row3 col6\" >0.667</td>\n",
       "      <td id=\"T_12e38_row3_col7\" class=\"data row3 col7\" >8.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row4_col0\" class=\"data row4 col0\" >Solon-Large-FT-Config1</td>\n",
       "      <td id=\"T_12e38_row4_col1\" class=\"data row4 col1\" >50.9</td>\n",
       "      <td id=\"T_12e38_row4_col2\" class=\"data row4 col2\" >76.2</td>\n",
       "      <td id=\"T_12e38_row4_col3\" class=\"data row4 col3\" >83.9</td>\n",
       "      <td id=\"T_12e38_row4_col4\" class=\"data row4 col4\" >90.0</td>\n",
       "      <td id=\"T_12e38_row4_col5\" class=\"data row4 col5\" >0.617</td>\n",
       "      <td id=\"T_12e38_row4_col6\" class=\"data row4 col6\" >0.577</td>\n",
       "      <td id=\"T_12e38_row4_col7\" class=\"data row4 col7\" >7.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row5_col0\" class=\"data row5 col0\" >Solon-Large-FT-Config4</td>\n",
       "      <td id=\"T_12e38_row5_col1\" class=\"data row5 col1\" >36.2</td>\n",
       "      <td id=\"T_12e38_row5_col2\" class=\"data row5 col2\" >66.8</td>\n",
       "      <td id=\"T_12e38_row5_col3\" class=\"data row5 col3\" >76.4</td>\n",
       "      <td id=\"T_12e38_row5_col4\" class=\"data row5 col4\" >82.6</td>\n",
       "      <td id=\"T_12e38_row5_col5\" class=\"data row5 col5\" >0.489</td>\n",
       "      <td id=\"T_12e38_row5_col6\" class=\"data row5 col6\" >0.467</td>\n",
       "      <td id=\"T_12e38_row5_col7\" class=\"data row5 col7\" >7.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row6_col0\" class=\"data row6 col0\" >Solon-Large-FT-Config3</td>\n",
       "      <td id=\"T_12e38_row6_col1\" class=\"data row6 col1\" >34.6</td>\n",
       "      <td id=\"T_12e38_row6_col2\" class=\"data row6 col2\" >59.3</td>\n",
       "      <td id=\"T_12e38_row6_col3\" class=\"data row6 col3\" >67.8</td>\n",
       "      <td id=\"T_12e38_row6_col4\" class=\"data row6 col4\" >75.1</td>\n",
       "      <td id=\"T_12e38_row6_col5\" class=\"data row6 col5\" >0.448</td>\n",
       "      <td id=\"T_12e38_row6_col6\" class=\"data row6 col6\" >0.437</td>\n",
       "      <td id=\"T_12e38_row6_col7\" class=\"data row6 col7\" >8.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_12e38_row7_col0\" class=\"data row7 col0\" >E5-Large-FT-Config1</td>\n",
       "      <td id=\"T_12e38_row7_col1\" class=\"data row7 col1\" >0.3</td>\n",
       "      <td id=\"T_12e38_row7_col2\" class=\"data row7 col2\" >1.1</td>\n",
       "      <td id=\"T_12e38_row7_col3\" class=\"data row7 col3\" >4.4</td>\n",
       "      <td id=\"T_12e38_row7_col4\" class=\"data row7 col4\" >11.8</td>\n",
       "      <td id=\"T_12e38_row7_col5\" class=\"data row7 col5\" >0.009</td>\n",
       "      <td id=\"T_12e38_row7_col6\" class=\"data row7 col6\" >0.011</td>\n",
       "      <td id=\"T_12e38_row7_col7\" class=\"data row7 col7\" >8.827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2dd722127d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d7598 th {\n",
       "  background-color: #111111;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_d7598 td {\n",
       "  border-color: #333333;\n",
       "}\n",
       "#T_d7598 caption {\n",
       "  caption-side: top;\n",
       "  color: white;\n",
       "  font-size: 14pt;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_d7598_row0_col0, #T_d7598_row1_col0, #T_d7598_row2_col0, #T_d7598_row3_col0, #T_d7598_row4_col0, #T_d7598_row5_col0, #T_d7598_row6_col0, #T_d7598_row7_col0 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background-color: #1f3c88;\n",
       "  color: white;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_d7598_row0_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 91%, #1e1e1e 91%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row0_col2, #T_d7598_row0_col3, #T_d7598_row1_col2, #T_d7598_row1_col3, #T_d7598_row2_col3, #T_d7598_row7_col4 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 99%, #1e1e1e 99%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row0_col4, #T_d7598_row1_col4, #T_d7598_row2_col4, #T_d7598_row3_col4, #T_d7598_row4_col4, #T_d7598_row5_col4, #T_d7598_row6_col4, #T_d7598_row7_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 100%, #1e1e1e 100%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row0_col5, #T_d7598_row0_col6, #T_d7598_row1_col5, #T_d7598_row1_col6, #T_d7598_row2_col5, #T_d7598_row2_col6, #T_d7598_row3_col5, #T_d7598_row3_col6, #T_d7598_row4_col5, #T_d7598_row4_col6, #T_d7598_row5_col5, #T_d7598_row5_col6, #T_d7598_row6_col5, #T_d7598_row6_col6, #T_d7598_row7_col5, #T_d7598_row7_col6 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 0%, #1e1e1e 0%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row0_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 6%, #1e1e1e 6%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row1_col1, #T_d7598_row3_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 82%, #1e1e1e 82%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row1_col7, #T_d7598_row2_col7, #T_d7598_row3_col7, #T_d7598_row7_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 8%, #1e1e1e 8%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row2_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 77%, #1e1e1e 77%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row2_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 98%, #1e1e1e 98%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row3_col1, #T_d7598_row7_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 34%, #1e1e1e 34%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row3_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 94%, #1e1e1e 94%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row4_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 33%, #1e1e1e 33%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row4_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 80%, #1e1e1e 80%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row4_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 92%, #1e1e1e 92%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row4_col7, #T_d7598_row6_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 9%, #1e1e1e 9%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row5_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 24%, #1e1e1e 24%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row5_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 74%, #1e1e1e 74%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row5_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 89%, #1e1e1e 89%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row5_col7 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 12%, #1e1e1e 12%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row6_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 21%, #1e1e1e 21%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row6_col2 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 71%, #1e1e1e 71%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row6_col3 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 88%, #1e1e1e 88%);\n",
       "  color: white;\n",
       "}\n",
       "#T_d7598_row7_col1 {\n",
       "  background-color: #1e1e1e;\n",
       "  color: white;\n",
       "  border-color: #333333;\n",
       "  text-align: center;\n",
       "  font-size: 12pt;\n",
       "  background: linear-gradient(90deg, #1f3c88 2%, #1e1e1e 2%);\n",
       "  color: white;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d7598\">\n",
       "  <caption>Benchmark DEF-ONLY (strict) - def du même cluster_id uniquement</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_d7598_level0_col0\" class=\"col_heading level0 col0\" >Modèle</th>\n",
       "      <th id=\"T_d7598_level0_col1\" class=\"col_heading level0 col1\" >R@1 (%)</th>\n",
       "      <th id=\"T_d7598_level0_col2\" class=\"col_heading level0 col2\" >R@5 (%)</th>\n",
       "      <th id=\"T_d7598_level0_col3\" class=\"col_heading level0 col3\" >R@10 (%)</th>\n",
       "      <th id=\"T_d7598_level0_col4\" class=\"col_heading level0 col4\" >R@20 (%)</th>\n",
       "      <th id=\"T_d7598_level0_col5\" class=\"col_heading level0 col5\" >MRR@10</th>\n",
       "      <th id=\"T_d7598_level0_col6\" class=\"col_heading level0 col6\" >nDCG@10</th>\n",
       "      <th id=\"T_d7598_level0_col7\" class=\"col_heading level0 col7\" >TotalCompute(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row0_col0\" class=\"data row0 col0\" >E5-Large-instruct</td>\n",
       "      <td id=\"T_d7598_row0_col1\" class=\"data row0 col1\" >91.3</td>\n",
       "      <td id=\"T_d7598_row0_col2\" class=\"data row0 col2\" >98.6</td>\n",
       "      <td id=\"T_d7598_row0_col3\" class=\"data row0 col3\" >99.2</td>\n",
       "      <td id=\"T_d7598_row0_col4\" class=\"data row0 col4\" >99.4</td>\n",
       "      <td id=\"T_d7598_row0_col5\" class=\"data row0 col5\" >0.946</td>\n",
       "      <td id=\"T_d7598_row0_col6\" class=\"data row0 col6\" >0.958</td>\n",
       "      <td id=\"T_d7598_row0_col7\" class=\"data row0 col7\" >7.563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row1_col0\" class=\"data row1 col0\" >E5-Large</td>\n",
       "      <td id=\"T_d7598_row1_col1\" class=\"data row1 col1\" >80.8</td>\n",
       "      <td id=\"T_d7598_row1_col2\" class=\"data row1 col2\" >97.4</td>\n",
       "      <td id=\"T_d7598_row1_col3\" class=\"data row1 col3\" >97.8</td>\n",
       "      <td id=\"T_d7598_row1_col4\" class=\"data row1 col4\" >98.2</td>\n",
       "      <td id=\"T_d7598_row1_col5\" class=\"data row1 col5\" >0.887</td>\n",
       "      <td id=\"T_d7598_row1_col6\" class=\"data row1 col6\" >0.910</td>\n",
       "      <td id=\"T_d7598_row1_col7\" class=\"data row1 col7\" >9.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row2_col0\" class=\"data row2 col0\" >Solon-Large</td>\n",
       "      <td id=\"T_d7598_row2_col1\" class=\"data row2 col1\" >76.2</td>\n",
       "      <td id=\"T_d7598_row2_col2\" class=\"data row2 col2\" >96.7</td>\n",
       "      <td id=\"T_d7598_row2_col3\" class=\"data row2 col3\" >97.6</td>\n",
       "      <td id=\"T_d7598_row2_col4\" class=\"data row2 col4\" >98.1</td>\n",
       "      <td id=\"T_d7598_row2_col5\" class=\"data row2 col5\" >0.859</td>\n",
       "      <td id=\"T_d7598_row2_col6\" class=\"data row2 col6\" >0.889</td>\n",
       "      <td id=\"T_d7598_row2_col7\" class=\"data row2 col7\" >9.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row3_col0\" class=\"data row3 col0\" >Solon-Large-FT-Config2</td>\n",
       "      <td id=\"T_d7598_row3_col1\" class=\"data row3 col1\" >31.1</td>\n",
       "      <td id=\"T_d7598_row3_col2\" class=\"data row3 col2\" >75.1</td>\n",
       "      <td id=\"T_d7598_row3_col3\" class=\"data row3 col3\" >85.4</td>\n",
       "      <td id=\"T_d7598_row3_col4\" class=\"data row3 col4\" >90.5</td>\n",
       "      <td id=\"T_d7598_row3_col5\" class=\"data row3 col5\" >0.488</td>\n",
       "      <td id=\"T_d7598_row3_col6\" class=\"data row3 col6\" >0.576</td>\n",
       "      <td id=\"T_d7598_row3_col7\" class=\"data row3 col7\" >8.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row4_col0\" class=\"data row4 col0\" >Solon-Large-FT-Config1</td>\n",
       "      <td id=\"T_d7598_row4_col1\" class=\"data row4 col1\" >28.0</td>\n",
       "      <td id=\"T_d7598_row4_col2\" class=\"data row4 col2\" >66.3</td>\n",
       "      <td id=\"T_d7598_row4_col3\" class=\"data row4 col3\" >75.8</td>\n",
       "      <td id=\"T_d7598_row4_col4\" class=\"data row4 col4\" >82.3</td>\n",
       "      <td id=\"T_d7598_row4_col5\" class=\"data row4 col5\" >0.443</td>\n",
       "      <td id=\"T_d7598_row4_col6\" class=\"data row4 col6\" >0.519</td>\n",
       "      <td id=\"T_d7598_row4_col7\" class=\"data row4 col7\" >7.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row5_col0\" class=\"data row5 col0\" >Solon-Large-FT-Config3</td>\n",
       "      <td id=\"T_d7598_row5_col1\" class=\"data row5 col1\" >16.9</td>\n",
       "      <td id=\"T_d7598_row5_col2\" class=\"data row5 col2\" >50.9</td>\n",
       "      <td id=\"T_d7598_row5_col3\" class=\"data row5 col3\" >61.2</td>\n",
       "      <td id=\"T_d7598_row5_col4\" class=\"data row5 col4\" >68.4</td>\n",
       "      <td id=\"T_d7598_row5_col5\" class=\"data row5 col5\" >0.312</td>\n",
       "      <td id=\"T_d7598_row5_col6\" class=\"data row5 col6\" >0.384</td>\n",
       "      <td id=\"T_d7598_row5_col7\" class=\"data row5 col7\" >8.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row6_col0\" class=\"data row6 col0\" >Solon-Large-FT-Config4</td>\n",
       "      <td id=\"T_d7598_row6_col1\" class=\"data row6 col1\" >15.9</td>\n",
       "      <td id=\"T_d7598_row6_col2\" class=\"data row6 col2\" >51.8</td>\n",
       "      <td id=\"T_d7598_row6_col3\" class=\"data row6 col3\" >63.8</td>\n",
       "      <td id=\"T_d7598_row6_col4\" class=\"data row6 col4\" >71.9</td>\n",
       "      <td id=\"T_d7598_row6_col5\" class=\"data row6 col5\" >0.309</td>\n",
       "      <td id=\"T_d7598_row6_col6\" class=\"data row6 col6\" >0.388</td>\n",
       "      <td id=\"T_d7598_row6_col7\" class=\"data row6 col7\" >7.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_d7598_row7_col0\" class=\"data row7 col0\" >E5-Large-FT-Config1</td>\n",
       "      <td id=\"T_d7598_row7_col1\" class=\"data row7 col1\" >0.3</td>\n",
       "      <td id=\"T_d7598_row7_col2\" class=\"data row7 col2\" >0.7</td>\n",
       "      <td id=\"T_d7598_row7_col3\" class=\"data row7 col3\" >3.1</td>\n",
       "      <td id=\"T_d7598_row7_col4\" class=\"data row7 col4\" >8.8</td>\n",
       "      <td id=\"T_d7598_row7_col5\" class=\"data row7 col5\" >0.007</td>\n",
       "      <td id=\"T_d7598_row7_col6\" class=\"data row7 col6\" >0.012</td>\n",
       "      <td id=\"T_d7598_row7_col7\" class=\"data row7 col7\" >8.827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2dfd8422390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONFIG\n",
    "INPUT_JSONL = Path(\"Dataset_Bercy_4k_lines.jsonl\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "K_EVAL = 20\n",
    "MRR_K = 10\n",
    "NDCG_K = 10\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "BLOCK_SIZE = 128 if DEVICE == \"cuda\" else 32\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# MODELS (Modèles d'embeddings)\n",
    "CANDIDATS = [\n",
    "    {\"name\": \"Solon-Large\", \"id\": \"OrdalieTech/SOLON-embeddings-large-0.1\"},\n",
    "    {\"name\": \"Solon-Large-FT-Config1\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config1_merged\")},\n",
    "    {\"name\": \"Solon-Large-FT-Config2\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config2_merged\")},\n",
    "    {\"name\": \"Solon-Large-FT-Config3\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config3_merged\")},\n",
    "    {\"name\": \"Solon-Large-FT-Config4\", \"id\": str(LOCAL_DIR / \"solon_large_finetuned_config4_merged\")},\n",
    "    {\"name\": \"E5-Large\", \"id\": \"intfloat/multilingual-e5-large\"},\n",
    "    {\"name\": \"E5-Large-instruct\", \"id\": \"intfloat/multilingual-e5-large-instruct\"},\n",
    "    {\"name\": \"E5-Large-FT-Config1\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config1_merged\")},\n",
    "    {\"name\": \"E5-Large-FT-Config2\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config2_merged\")},\n",
    "    {\"name\": \"E5-Large-FT-Config3\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config3_merged\")},\n",
    "    {\"name\": \"E5-Large-FT-Config4\", \"id\": str(LOCAL_DIR / \"e5_large_finetuned_config4_merged\")},\n",
    "]\n",
    "\n",
    "def prefixes_for(model_name: str):\n",
    "    if \"E5-Large-instruct\" in model_name or \"E5-Large-FT\" in model_name:\n",
    "        return INSTRUCT_Q, \"\"\n",
    "    if \"E5\" in model_name:\n",
    "        return \"query: \", \"passage: \"\n",
    "    if \"Snowflake\" in model_name:\n",
    "        return \"query: \", \"\"\n",
    "    return \"\", \"\"\n",
    "\n",
    "def norm_space(s: str) -> str:\n",
    "    return \" \".join(str(s).strip().split())\n",
    "\n",
    "def cuda_empty_cache():\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class SnowflakeWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper pour utiliser Snowflake (HF transformers) avec une API encode() proche SentenceTransformer.\n",
    "    Mean pooling + normalisation L2.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        if hasattr(config, \"use_memory_efficient_attention\"):\n",
    "            config.use_memory_efficient_attention = False\n",
    "        if hasattr(config, \"unpad_inputs\"):\n",
    "            config.unpad_inputs = False\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            add_pooling_layer=False,\n",
    "            trust_remote_code=True\n",
    "        ).to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        mask = attention_mask.unsqueeze(-1).type_as(last_hidden_state)\n",
    "        summed = (last_hidden_state * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-9)\n",
    "        return summed / denom\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        prompt_name: Optional[str] = None,\n",
    "        batch_size: int = 32,\n",
    "        prefix_query: str = \"query: \",\n",
    "        prefix_doc: str = \"\"\n",
    "    ) -> torch.Tensor:\n",
    "        prefix = prefix_query if prompt_name == \"query\" else prefix_doc\n",
    "        inputs = [prefix + s for s in sentences]\n",
    "\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch_texts = inputs[i:i + batch_size]\n",
    "            batch_tokens = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=MAX_LENGTH\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model(**batch_tokens)\n",
    "                emb = self._mean_pooling(outputs[0], batch_tokens[\"attention_mask\"])\n",
    "                emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
    "                all_embeddings.append(emb)\n",
    "\n",
    "        return torch.cat(all_embeddings, dim=0) if all_embeddings else torch.empty((0, 0), device=DEVICE)\n",
    "\n",
    "# 1) LOAD JSONL\n",
    "rows = []\n",
    "with INPUT_JSONL.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "print(f\"Loaded rows: {len(rows)}\")\n",
    "\n",
    "# 2) BUILD DOCS + QUERIES\n",
    "DOCS: List[str] = []\n",
    "DOC_META: List[dict] = []\n",
    "DOC_INDEX_BY_CLUSTER_AND_TYPE: Dict[Tuple[str, str], int] = {}  # (cluster_id, pair_type) -> doc_idx\n",
    "DOC_INDICES_BY_CLUSTER: Dict[str, List[int]] = defaultdict(list)\n",
    "\n",
    "QUERIES: List[str] = []\n",
    "Q_META: List[dict] = []\n",
    "\n",
    "# Docs: one doc for each (cluster_id, def) and (cluster_id, context)\n",
    "for r in rows:\n",
    "    cid = r.get(\"cluster_id\")\n",
    "    ptype = r.get(\"pair_type\")\n",
    "    pos = norm_space(r.get(\"positive\", \"\"))\n",
    "    if not cid or ptype not in {\"def\", \"context\"} or not pos:\n",
    "        continue\n",
    "\n",
    "    key = (cid, ptype)\n",
    "    if key in DOC_INDEX_BY_CLUSTER_AND_TYPE:\n",
    "        continue\n",
    "\n",
    "    doc_idx = len(DOCS)\n",
    "    DOCS.append(pos)\n",
    "    DOC_META.append({\"cluster_id\": cid, \"pair_type\": ptype})\n",
    "    DOC_INDEX_BY_CLUSTER_AND_TYPE[key] = doc_idx\n",
    "    DOC_INDICES_BY_CLUSTER[cid].append(doc_idx)\n",
    "\n",
    "print(f\"Docs built (def+context): {len(DOCS)}\")\n",
    "\n",
    "# Queries:\n",
    "# - from def/context: anchor keyword + \"C'est quoi\"\n",
    "# - from qa: anchor (question)\n",
    "for r in rows:\n",
    "    cid = r.get(\"cluster_id\")\n",
    "    ptype = r.get(\"pair_type\")\n",
    "    anchor = norm_space(r.get(\"anchor\", \"\"))\n",
    "\n",
    "    if not cid or not anchor:\n",
    "        continue\n",
    "\n",
    "    if ptype in {\"def\", \"context\"}:\n",
    "        QUERIES.append(anchor)\n",
    "        Q_META.append({\"cluster_id\": cid, \"kind\": \"anchor_keyword\", \"source_pair_type\": ptype})\n",
    "\n",
    "        QUERIES.append(f\"C'est quoi {anchor} ?\")\n",
    "        Q_META.append({\"cluster_id\": cid, \"kind\": \"anchor_cestquoi\", \"source_pair_type\": ptype})\n",
    "\n",
    "    elif ptype == \"qa\":\n",
    "        QUERIES.append(anchor)\n",
    "        Q_META.append({\"cluster_id\": cid, \"kind\": \"qa_question\", \"source_pair_type\": \"qa\"})\n",
    "\n",
    "print(f\"Queries built: {len(QUERIES)}\")\n",
    "\n",
    "# 3) GROUND TRUTH (2 versions)\n",
    "GT_CONCEPT: List[Set[int]] = []\n",
    "GT_DEFONLY: List[Set[int]] = []\n",
    "\n",
    "missing_concept = 0\n",
    "missing_def = 0\n",
    "\n",
    "for qm in Q_META:\n",
    "    cid = qm[\"cluster_id\"]\n",
    "\n",
    "    rel_concept = set(DOC_INDICES_BY_CLUSTER.get(cid, []))  # def + context\n",
    "    if not rel_concept:\n",
    "        missing_concept += 1\n",
    "    GT_CONCEPT.append(rel_concept)\n",
    "\n",
    "    def_idx = DOC_INDEX_BY_CLUSTER_AND_TYPE.get((cid, \"def\"))\n",
    "    if def_idx is None:\n",
    "        missing_def += 1\n",
    "        GT_DEFONLY.append(set())\n",
    "    else:\n",
    "        GT_DEFONLY.append({def_idx})\n",
    "\n",
    "print(f\"Missing concept GT: {missing_concept}\")\n",
    "print(f\"Missing def-only GT: {missing_def}\")\n",
    "\n",
    "# 4) RETRIEVAL TOPK (blockwise)\n",
    "def compute_topk_blockwise(emb_q: torch.Tensor, emb_d: torch.Tensor, k_eval: int, block_size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    emb_q: [Nq, dim], emb_d: [Nd, dim]\n",
    "    returns indices: [Nq, K]\n",
    "    \"\"\"\n",
    "    Nq = emb_q.size(0)\n",
    "    K = min(k_eval, emb_d.size(0))\n",
    "    emb_d_t = emb_d.T\n",
    "\n",
    "    out = []\n",
    "    for i in range(0, Nq, block_size):\n",
    "        q_block = emb_q[i:i + block_size]\n",
    "        scores = torch.matmul(q_block, emb_d_t)  # cosine if normalized\n",
    "        topk = torch.topk(scores, k=K, dim=1).indices\n",
    "        out.append(topk.detach().cpu())\n",
    "        del scores, topk, q_block\n",
    "        cuda_empty_cache()\n",
    "\n",
    "    return torch.cat(out, dim=0)\n",
    "\n",
    "def compute_metrics_from_topk(\n",
    "    topk_indices: torch.Tensor,\n",
    "    ground_truth: List[Set[int]],\n",
    "    mrr_k: int,\n",
    "    ndcg_k: int\n",
    ") -> Dict[str, float]:\n",
    "    Nq, K = topk_indices.shape\n",
    "    k5 = min(5, K)\n",
    "    k10 = min(10, K)\n",
    "    k20 = min(20, K)\n",
    "    mrr_k = min(mrr_k, K)\n",
    "    ndcg_k = min(ndcg_k, K)\n",
    "\n",
    "    r1 = r5 = r10 = r20 = 0\n",
    "    mrr = 0.0\n",
    "    ndcg = 0.0\n",
    "\n",
    "    for i in range(Nq):\n",
    "        relevant = ground_truth[i]\n",
    "        ranked = topk_indices[i].tolist()\n",
    "\n",
    "        if relevant:\n",
    "            if ranked[0] in relevant:\n",
    "                r1 += 1\n",
    "            if any(idx in relevant for idx in ranked[:k5]):\n",
    "                r5 += 1\n",
    "            if any(idx in relevant for idx in ranked[:k10]):\n",
    "                r10 += 1\n",
    "            if any(idx in relevant for idx in ranked[:k20]):\n",
    "                r20 += 1\n",
    "\n",
    "            rr = 0.0\n",
    "            for rank_pos, doc_idx in enumerate(ranked[:mrr_k], start=1):\n",
    "                if doc_idx in relevant:\n",
    "                    rr = 1.0 / rank_pos\n",
    "                    break\n",
    "            mrr += rr\n",
    "\n",
    "            dcg = 0.0\n",
    "            for rank_pos, doc_idx in enumerate(ranked[:ndcg_k], start=1):\n",
    "                if doc_idx in relevant:\n",
    "                    dcg += 1.0 / np.log2(rank_pos + 1)\n",
    "\n",
    "            rel_count = min(len(relevant), ndcg_k)\n",
    "            idcg = 0.0\n",
    "            for rank_pos in range(1, rel_count + 1):\n",
    "                idcg += 1.0 / np.log2(rank_pos + 1)\n",
    "\n",
    "            ndcg += (dcg / idcg) if idcg > 0 else 0.0\n",
    "        else:\n",
    "            # si GT vide pour cette query : on ignore (ou alors ça pénalise)\n",
    "            # ici on pénalise implicitement car rr=0, recalls=0, ndcg=0\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"N_queries\": Nq,\n",
    "        \"R@1 (%)\": (r1 / Nq) * 100,\n",
    "        \"R@5 (%)\": (r5 / Nq) * 100,\n",
    "        \"R@10 (%)\": (r10 / Nq) * 100,\n",
    "        \"R@20 (%)\": (r20 / Nq) * 100,\n",
    "        f\"MRR@{mrr_k}\": mrr / Nq,\n",
    "        f\"nDCG@{ndcg_k}\": ndcg / Nq,\n",
    "    }\n",
    "\n",
    "# 5) ENCODE HELPERS\n",
    "def encode_st(model: SentenceTransformer, texts: List[str], prefix: str) -> torch.Tensor:\n",
    "    emb = model.encode(\n",
    "        [prefix + t for t in texts],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True,   # cosine = dot product\n",
    "    )\n",
    "    return emb\n",
    "\n",
    "\n",
    "# 6) LOAD & ENCODE MODEL HELPERS\n",
    "def load_model_smart(cand):\n",
    "    t0 = time.perf_counter()\n",
    "    model = None\n",
    "    err = None\n",
    "\n",
    "    try:\n",
    "        if \"Snowflake\" in cand[\"name\"]:\n",
    "            model = SnowflakeWrapper(cand[\"id\"])\n",
    "        else:\n",
    "            kw = {\"torch_dtype\": torch.float16} if DEVICE == \"cuda\" else {}\n",
    "            trust = cand.get(\"trust\", False)\n",
    "            model = SentenceTransformer(cand[\"id\"], trust_remote_code=trust, device=DEVICE, model_kwargs=kw)\n",
    "    except Exception as e:\n",
    "        err = e\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    return model, (t1 - t0), err\n",
    "\n",
    "\n",
    "def encode_any(model, model_name: str, texts: List[str], prefix: str, is_query: bool) -> torch.Tensor:\n",
    "    # Snowflake : wrapper transformers\n",
    "    if \"Snowflake\" in model_name:\n",
    "        return model.encode(\n",
    "            texts,\n",
    "            prompt_name=\"query\" if is_query else None,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            prefix_query=prefix,\n",
    "            prefix_doc=prefix\n",
    "        )\n",
    "    # SentenceTransformer\n",
    "    return model.encode(\n",
    "        [prefix + t for t in texts],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "\n",
    "# 7) RUN BENCHMARK\n",
    "results_concept = []\n",
    "results_defonly = []\n",
    "\n",
    "for cand in CANDIDATS:\n",
    "    gc.collect()\n",
    "    cuda_empty_cache()\n",
    "\n",
    "    name = cand[\"name\"]\n",
    "    mid = cand[\"id\"]\n",
    "    trust = cand.get(\"trust\", False)\n",
    "\n",
    "    print(f\"\\nLoading: {name} ({mid}) ...\")\n",
    "    model, load_s, err = load_model_smart(cand)\n",
    "    if err is not None or model is None:\n",
    "        print(f\"Load error for {name}: {err}\")\n",
    "        continue\n",
    "\n",
    "    pq, pd_ = model_prefixes(name)\n",
    "\n",
    "    # Encode queries\n",
    "    t0 = time.perf_counter()\n",
    "    emb_q = encode_any(model, name, QUERIES, pq, is_query=True)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Encode docs\n",
    "    emb_d = encode_any(model, name, DOCS, pd_, is_query=False)\n",
    "    t2 = time.perf_counter()\n",
    "\n",
    "    # Retrieval\n",
    "    t3 = time.perf_counter()\n",
    "    topk = compute_topk_blockwise(emb_q.to(DEVICE), emb_d.to(DEVICE), k_eval=K_EVAL, block_size=BLOCK_SIZE)\n",
    "    t4 = time.perf_counter()\n",
    "\n",
    "    # Metrics\n",
    "    m_concept = compute_metrics_from_topk(topk, GT_CONCEPT, mrr_k=MRR_K, ndcg_k=NDCG_K)\n",
    "    m_defonly = compute_metrics_from_topk(topk, GT_DEFONLY, mrr_k=MRR_K, ndcg_k=NDCG_K)\n",
    "\n",
    "    # Timings\n",
    "    encq_s = t1 - t0\n",
    "    encd_s = t2 - t1\n",
    "    retr_s = t4 - t3\n",
    "    total_s = t4 - t0\n",
    "\n",
    "    row_common = {\n",
    "        \"Modèle\": name,\n",
    "        \"Load(s)\": load_s,\n",
    "        \"EncQ(s)\": encq_s,\n",
    "        \"EncD(s)\": encd_s,\n",
    "        \"Retr(s)\": retr_s,\n",
    "        \"TotalCompute(s)\": total_s,\n",
    "        \"Nq\": m_concept[\"N_queries\"],\n",
    "        \"Nd\": len(DOCS),\n",
    "        \"K\": min(K_EVAL, len(DOCS)),\n",
    "    }\n",
    "\n",
    "    results_concept.append({**row_common, **{k: v for k, v in m_concept.items() if k != \"N_queries\"}})\n",
    "    results_defonly.append({**row_common, **{k: v for k, v in m_defonly.items() if k != \"N_queries\"}})\n",
    "\n",
    "    print(\n",
    "        f\"{name} | Concept R@1={results_concept[-1]['R@1 (%)']:.1f}% \"\n",
    "        f\"| DefOnly R@1={results_defonly[-1]['R@1 (%)']:.1f}% \"\n",
    "        f\"| Total={total_s:.1f}s\"\n",
    "    )\n",
    "\n",
    "    del model, emb_q, emb_d\n",
    "    gc.collect()\n",
    "    cuda_empty_cache()\n",
    "\n",
    "# 8) FONCTIONS DISPLAY\n",
    "def row_gradient(val, row_min, row_max):\n",
    "    if row_max == row_min:\n",
    "        pct = 0\n",
    "    else:\n",
    "        pct = (val - row_min) / (row_max - row_min)\n",
    "    pct = int(pct * 100)\n",
    "\n",
    "    return (\n",
    "        f\"background: linear-gradient(90deg, \"\n",
    "        f\"#1f3c88 {pct}%, \"\n",
    "        f\"#1e1e1e {pct}%);\"\n",
    "        f\"color: white;\"\n",
    "    )\n",
    "\n",
    "def apply_rowwise_gradient(df, cols):\n",
    "    styles = pd.DataFrame(\"\", index=df.index, columns=df.columns)\n",
    "    for idx in df.index:\n",
    "        row_vals = df.loc[idx, cols].astype(float)\n",
    "        rmin, rmax = row_vals.min(), row_vals.max()\n",
    "        for col in cols:\n",
    "            styles.loc[idx, col] = row_gradient(df.loc[idx, col], rmin, rmax)\n",
    "    return styles\n",
    "\n",
    "def display_ranked_table(df_sorted: pd.DataFrame, title: str):\n",
    "    # colonnes attendues\n",
    "    gradient_cols = [\n",
    "        \"R@1 (%)\",\n",
    "        \"R@5 (%)\",\n",
    "        \"R@10 (%)\",\n",
    "        \"R@20 (%)\",\n",
    "        f\"MRR@{MRR_K}\",\n",
    "        f\"nDCG@{NDCG_K}\",\n",
    "        \"TotalCompute(s)\",\n",
    "    ]\n",
    "    gradient_cols = [c for c in gradient_cols if c in df_sorted.columns]\n",
    "\n",
    "    df_display = df_sorted[[\"Modèle\"] + gradient_cols].copy().reset_index(drop=True)\n",
    "\n",
    "    fmt = {\n",
    "        \"R@1 (%)\": \"{:.1f}\",\n",
    "        \"R@5 (%)\": \"{:.1f}\",\n",
    "        \"R@10 (%)\": \"{:.1f}\",\n",
    "        \"R@20 (%)\": \"{:.1f}\",\n",
    "        f\"MRR@{MRR_K}\": \"{:.3f}\",\n",
    "        f\"nDCG@{NDCG_K}\": \"{:.3f}\",\n",
    "        \"TotalCompute(s)\": \"{:.3f}\",\n",
    "    }\n",
    "    fmt = {k: v for k, v in fmt.items() if k in df_display.columns}\n",
    "\n",
    "    styled = (\n",
    "        df_display.style\n",
    "        .format(fmt)\n",
    "        .hide(axis=\"index\")\n",
    "        .set_caption(title)\n",
    "        .set_properties(**{\n",
    "            \"background-color\": \"#1e1e1e\",\n",
    "            \"color\": \"white\",\n",
    "            \"border-color\": \"#333333\",\n",
    "            \"text-align\": \"center\",\n",
    "            \"font-size\": \"12pt\"\n",
    "        })\n",
    "        .set_table_styles([\n",
    "            {\"selector\": \"th\", \"props\": [\n",
    "                (\"background-color\", \"#111111\"),\n",
    "                (\"color\", \"white\"),\n",
    "                (\"border-color\", \"#333333\"),\n",
    "                (\"text-align\", \"center\")\n",
    "            ]},\n",
    "            {\"selector\": \"td\", \"props\": [\n",
    "                (\"border-color\", \"#333333\")\n",
    "            ]},\n",
    "            {\"selector\": \"caption\", \"props\": [\n",
    "                (\"caption-side\", \"top\"),\n",
    "                (\"color\", \"white\"),\n",
    "                (\"font-size\", \"14pt\"),\n",
    "                (\"font-weight\", \"bold\")\n",
    "            ]}\n",
    "        ])\n",
    "        .apply(lambda _: apply_rowwise_gradient(df_display, gradient_cols), axis=None)\n",
    "    )\n",
    "\n",
    "    # Bleu uniquement sur la colonne Modèle\n",
    "    styled = styled.set_properties(\n",
    "        subset=[\"Modèle\"],\n",
    "        **{\n",
    "            \"background-color\": \"#1f3c88\",\n",
    "            \"color\": \"white\",\n",
    "            \"font-weight\": \"bold\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display(styled)\n",
    "\n",
    "# 9) DISPLAY TABLES\n",
    "df_concept = pd.DataFrame(results_concept)\n",
    "df_defonly = pd.DataFrame(results_defonly)\n",
    "\n",
    "df_concept_sorted = df_concept.sort_values(by=\"R@1 (%)\", ascending=False).reset_index(drop=True)\n",
    "df_defonly_sorted = df_defonly.sort_values(by=\"R@1 (%)\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "display_ranked_table(df_concept_sorted, \"Benchmark CONCEPT (tolérant) - même cluster_id (def ou context)\")\n",
    "display_ranked_table(df_defonly_sorted, \"Benchmark DEF-ONLY (strict) - def du même cluster_id uniquement\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
