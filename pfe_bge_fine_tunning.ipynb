{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMD31n0UOgwGUvxkS/EJ9sb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *0. Contexte*"
      ],
      "metadata": {
        "id": "rNZoMcx26Gtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BGE est un modèle performant pour la recherche documentaire et dans le classement des bases de données massives. Il est principalement utilisé dans la recherche scientifique, la finance, et la pharmacie pour organiser et classer de grands volumes d’information."
      ],
      "metadata": {
        "id": "lr5d9_6V6XdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LegalKit contient des paires query ↔ document prêtes pour l’entraînement d’embeddings en droit français (≈53k lignes, licence CC-BY-4.0).\n",
        "Colonnes typiques : query (question), input (passage légal), + métadonnées."
      ],
      "metadata": {
        "id": "gCcnawjt6PS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. *Importation de librairies*"
      ],
      "metadata": {
        "id": "wkPSAE7b6f8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRnJwMfH8Iao"
      },
      "outputs": [],
      "source": [
        "pip install -U FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets sentence-transformers faiss-cpu accelerate peft bitsandbytes"
      ],
      "metadata": {
        "id": "cTUfFKkq6sHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. *Dataset*\n"
      ],
      "metadata": {
        "id": "EFq6eAGw6z3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"louisbrulenaudet/legalkit\")"
      ],
      "metadata": {
        "id": "9crXppTl66dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. *Nettoyage & mapping colonnes → format QA (query, positive)*"
      ],
      "metadata": {
        "id": "NcrBnLqx7MXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "import pandas as pd\n",
        "\n",
        "# 1. On garde les colonnes dont on a besoin\n",
        "ds_legalkit = ds[\"train\"].select_columns([\"query\", \"input\", \"output\"])\n",
        "ds_legalkit\n",
        "\n",
        "# 2) Concaténer input + output -> \"positive\" (avec nettoyage + limite longueur)\n",
        "ds_legalkit = ds_legalkit.map(\n",
        "    lambda ex: {\"query\": ex[\"query\"],\n",
        "                \"positive\": f\"{ex['input'].strip()}, {ex['output'].strip()}\"},\n",
        "    remove_columns=[\"input\", \"output\"]\n",
        ")\n",
        "#2.1 Afficher un aperçu (5 lignes)\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "display(ds_legalkit.select(range(5)).to_pandas())\n",
        "\n",
        "# 3. enlever lignes vides\n",
        "nb_vides = sum(1 for ex in ds_legalkit if not ex[\"query\"] or not ex[\"positive\"])\n",
        "print(\"\\nNombre de lignes vides : \", nb_vides)\n",
        "\n",
        "# 4. Splitter en 70/15/15\n",
        "ds_legalkit_dev  = ds_legalkit.train_test_split(test_size=0.30, seed=42, shuffle=True)  # 70% / 30%\n",
        "ds_legalkit_test = ds_legalkit_dev[\"test\"].train_test_split(test_size=0.5, seed=42, shuffle=True)  # 15% / 15%\n",
        "\n",
        "splits = DatasetDict({\n",
        "    \"train\": ds_legalkit_dev[\"train\"],\n",
        "    \"dev\":   ds_legalkit_test[\"train\"],   # (= validation)\n",
        "    \"test\":  ds_legalkit_test[\"test\"],\n",
        "})\n",
        "\n",
        "# 5. Vérifier les tailles\n",
        "print(\"\\n\", splits)\n",
        "for name in [\"train\",\"dev\",\"test\"]:\n",
        "    print(\"\\n\", name + \" :\", len(splits[name]))"
      ],
      "metadata": {
        "id": "LYMQOsqf7Sjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. *Zero-shot avec BGE-M3 (+ Weights & Biases)*"
      ],
      "metadata": {
        "id": "Vdwt6mSJ1l4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "dSSUUFKK3ZTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U tqdm"
      ],
      "metadata": {
        "id": "ERhCp3LKIhjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Vérifier que Colab utilise bien le GPU (et pas le CPU)"
      ],
      "metadata": {
        "id": "uhsXb0hhNSzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, transformers, sentence_transformers\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    raise SystemExit(\"⚠️ Pas de GPU actif. Va dans Runtime > Change runtime type > GPU puis redémarre.\")"
      ],
      "metadata": {
        "id": "wzSE2vnINI4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Petites optimisations globales (GPU + tokenizers)"
      ],
      "metadata": {
        "id": "mhtKie-oNWyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "jjlE7VWOPPJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3 Charger le modèle sur GPU et réduire un peu la longueur max"
      ],
      "metadata": {
        "id": "iw-tYoWDPTer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_NAME = \"BAAI/bge-m3\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "\n",
        "# raccourcir un peu pour accélérer l'encodage (optionnel, bon pour un baseline)\n",
        "model.max_seq_length = 384  # 512 par défaut. 384 = +rapide, souvent même score"
      ],
      "metadata": {
        "id": "AehE6SnZRB4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.4 Fonction d’encodage"
      ],
      "metadata": {
        "id": "Jp9uERK2RFO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time, numpy as np, os\n",
        "\n",
        "CACHE_DIR = \"./cache_zero_shot_test\"\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "def encode_texts(texts, tag, use_cache=True):\n",
        "    cache_file = os.path.join(CACHE_DIR, f\"{tag}_{len(texts)}.npy\")  # inclut la taille -> pas de mismatch\n",
        "    if use_cache and os.path.exists(cache_file):\n",
        "        emb = np.load(cache_file)\n",
        "        if emb.shape[0] == len(texts):\n",
        "            print(f\"[cache] loaded: {cache_file}\")\n",
        "            return emb\n",
        "\n",
        "    bs = 256 if device==\"cuda\" else 32   # T4 tient souvent 256 sur BGE-M3; ajuste si OOM\n",
        "    t0 = time.time()\n",
        "    emb = model.encode(\n",
        "        texts,\n",
        "        batch_size=bs,\n",
        "        normalize_embeddings=True,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,   # évite copies\n",
        "    )\n",
        "    if use_cache:\n",
        "        np.save(cache_file, emb)\n",
        "        print(f\"[cache] saved: {cache_file} ({emb.shape}) in {time.time()-t0:.1f}s\")\n",
        "    return emb"
      ],
      "metadata": {
        "id": "ODMS_zKVRSIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *5. Ré-exécuter le zéro-shot avec GPU*"
      ],
      "metadata": {
        "id": "74hZPmgNRV4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5.1 Setup commun (modèle + données)*"
      ],
      "metadata": {
        "id": "wD4FpJxRhwLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, os, numpy as np, torch, faiss\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ----- Modèle -----\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)\n",
        "model = SentenceTransformer(\"BAAI/bge-m3\", device=device)\n",
        "model.max_seq_length = 384  # un peu plus rapide que 512 pour un baseline\n",
        "bs = 256 if device==\"cuda\" else 32\n",
        "\n",
        "# ----- Données (TEST only) -----\n",
        "test_q = list(splits[\"test\"][\"query\"])\n",
        "test_p = list(splits[\"test\"][\"positive\"])\n",
        "\n",
        "print(\"#queries:\", len(test_q))"
      ],
      "metadata": {
        "id": "bMOcxI8jhqsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5.2 Pairwise — “query vs son positive”*"
      ],
      "metadata": {
        "id": "LPZMRkmbh3md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Embeddings alignés (même ordre)\n",
        "q_emb = model.encode(test_q, batch_size=bs, normalize_embeddings=True, show_progress_bar=True)\n",
        "p_emb = model.encode(test_p, batch_size=bs, normalize_embeddings=True, show_progress_bar=True)\n",
        "\n",
        "# 2) Cosine pairwise (produit scalaire car normalisé)\n",
        "cos_pos = (q_emb * p_emb).sum(axis=1)\n",
        "\n",
        "print(\"Pairwise cosine:\")\n",
        "print(\"  mean:\", float(np.mean(cos_pos)))\n",
        "print(\"  median:\", float(np.median(cos_pos)))\n",
        "print(\"  min/max:\", float(np.min(cos_pos)), float(np.max(cos_pos)))\n",
        "\n",
        "# 3) Petit sanity-check avec un négatif aléatoire (1 par query)\n",
        "rng = np.random.default_rng(123)\n",
        "neg_idx = rng.integers(0, len(test_p), size=len(test_p))\n",
        "# évite de choisir le gold lui-même\n",
        "neg_idx = np.where(neg_idx == np.arange(len(test_p)), (neg_idx+1) % len(test_p), neg_idx)\n",
        "neg_emb = p_emb[neg_idx]\n",
        "\n",
        "cos_neg = (q_emb * neg_emb).sum(axis=1)\n",
        "\n",
        "pairwise_acc = float(np.mean(cos_pos > cos_neg))  # % de queries où le positif > négatif\n",
        "print(\"Pairwise accuracy (pos > 1 random neg):\", round(pairwise_acc, 3))\n",
        "\n",
        "# 4) (Option) mini-retrieval local pour chaque query avec 1 positif + N négatifs aléatoires\n",
        "N_NEG = 19  # 1 positif + 19 négatifs => top-20\n",
        "hits = 0\n",
        "for i in range(len(test_q)):\n",
        "    cand_idx = set([i])\n",
        "    while len(cand_idx) < N_NEG+1:\n",
        "        j = int(rng.integers(0, len(test_p)))\n",
        "        if j != i:\n",
        "            cand_idx.add(j)\n",
        "    cand_idx = list(cand_idx)\n",
        "    cand_emb = p_emb[cand_idx]                          # [N, d]\n",
        "    sims = cand_emb @ q_emb[i]                          # [N]\n",
        "    # rang du vrai positif (son index local dans cand_idx)\n",
        "    true_local = cand_idx.index(i)\n",
        "    rank = (np.argsort(-sims).tolist()).index(true_local) + 1\n",
        "    hits += (rank == 1)\n",
        "\n",
        "mini_retrieval_R1 = hits/len(test_q)\n",
        "print(f\"Mini-retrieval@1 (1 pos + {N_NEG} neg aléatoires):\", round(mini_retrieval_R1, 3))"
      ],
      "metadata": {
        "id": "iT51lUzHiDNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5.3 Retrieval réaliste — FAISS sur un corpus (métriques IR)*"
      ],
      "metadata": {
        "id": "529wWENHiHVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Corpus = toutes les positives du test, dédupliquées\n",
        "corpus = list(set(test_p))\n",
        "\n",
        "# 2) Encodage corpus + queries (réutilise q_emb si identique)\n",
        "corpus_emb = model.encode(corpus, batch_size=bs, normalize_embeddings=True, show_progress_bar=True)\n",
        "query_emb  = q_emb  # déjà calculé au-dessus; sinon: model.encode(test_q, ...)\n",
        "\n",
        "# 3) FAISS index (cosine via inner product sur vecteurs normalisés)\n",
        "index = faiss.IndexFlatIP(corpus_emb.shape[1])\n",
        "index.add(corpus_emb)\n",
        "D, I = index.search(query_emb, 10)  # indices top-10 dans 'corpus' pour chaque query\n",
        "\n",
        "# 4) Associer chaque gold (test_p[i]) à son index dans 'corpus' (via normalisation de texte)\n",
        "def norm(s: str) -> str:\n",
        "    return \" \".join(s.split()).strip()\n",
        "\n",
        "corpus_norm = [norm(x) for x in corpus]\n",
        "gold_map = {corpus_norm[i]: i for i in range(len(corpus))}\n",
        "gold_indices = [gold_map.get(norm(g), -1) for g in test_p]  # -1 si absent (ça ne devrait pas arriver)\n",
        "\n",
        "# 5) Métriques IR\n",
        "def recall_at_k_idx(I, k, golds):\n",
        "    hits, total = 0, 0\n",
        "    for row_idx, gi in enumerate(golds):\n",
        "        if gi == -1:\n",
        "            continue\n",
        "        total += 1\n",
        "        if gi in I[row_idx, :k]:\n",
        "            hits += 1\n",
        "    return hits / max(1,total)\n",
        "\n",
        "def mrr_at_k_idx(I, k, golds):\n",
        "    tot, n = 0.0, 0\n",
        "    for row_idx, gi in enumerate(golds):\n",
        "        if gi == -1:\n",
        "            continue\n",
        "        n += 1\n",
        "        row = list(I[row_idx, :k])\n",
        "        if gi in row:\n",
        "            tot += 1.0 / (row.index(gi) + 1)\n",
        "    return tot / max(1,n)\n",
        "\n",
        "def ndcg_at_k_idx(I, k, golds):\n",
        "    tot, n = 0.0, 0\n",
        "    for row_idx, gi in enumerate(golds):\n",
        "        if gi == -1:\n",
        "            continue\n",
        "        n += 1\n",
        "        gains = [1.0 if j == gi else 0.0 for j in I[row_idx, :k]]\n",
        "        dcg = sum(g / math.log2(i + 2) for i, g in enumerate(gains))\n",
        "        tot += dcg  # IDCG = 1 (un seul pertinent)\n",
        "    return tot / max(1,n)\n",
        "\n",
        "R1   = recall_at_k_idx(I, 1,  gold_indices)\n",
        "R5   = recall_at_k_idx(I, 5,  gold_indices)\n",
        "R10  = recall_at_k_idx(I, 10, gold_indices)\n",
        "MRR10  = mrr_at_k_idx(I, 10, gold_indices)\n",
        "NDCG10 = ndcg_at_k_idx(I, 10, gold_indices)\n",
        "\n",
        "print({\n",
        "    \"R@1\": round(R1,3), \"R@5\": round(R5,3), \"R@10\": round(R10,3),\n",
        "    \"MRR@10\": round(MRR10,3), \"nDCG@10\": round(NDCG10,3),\n",
        "    \"n_queries\": len(test_q), \"n_docs\": len(corpus)\n",
        "})"
      ],
      "metadata": {
        "id": "sGccdnz5ac03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concrètement, pour bien comprendre :\n",
        "\n",
        "Pairwise : “le modèle met-il bien la query proche de son positive ?” (cosines, acc vs neg aléatoire).\n",
        "\n",
        "Retrieval : “parmi des centaines/milliers de passages, la bonne réponse ressort-elle dans le top-k ?”."
      ],
      "metadata": {
        "id": "Tz0N9_Deif2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*5.4 Weights & Biases (R@1/5/10, MRR, nDCG@10)*"
      ],
      "metadata": {
        "id": "bNeBWEK4atS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "run = wandb.init(project=\"legal-embeddings\", name=\"bge-m3_zero-shot_test_pairwise+retrieval\")\n",
        "\n",
        "wandb.log({\n",
        "  \"pairwise/mean_cos\": float(np.mean(cos_pos)),\n",
        "  \"pairwise/acc_pos>neg\": pairwise_acc,\n",
        "  \"mini_retrieval/R@1_(1pos+19neg)\": mini_retrieval_R1,\n",
        "  \"retrieval/R@1\": R1, \"retrieval/R@5\": R5, \"retrieval/R@10\": R10,\n",
        "  \"retrieval/MRR@10\": MRR10, \"retrieval/nDCG@10\": NDCG10,\n",
        "  \"n_queries\": len(test_q), \"n_docs\": len(corpus)\n",
        "})\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "jIl5RrAeasZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Fine-tuning avec LoRA"
      ],
      "metadata": {
        "id": "o9lEhsYt6G35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1 Prépare les données + évaluateur (dev)"
      ],
      "metadata": {
        "id": "c6oGUDmPl9tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "from sentence_transformers import losses, evaluation\n",
        "import numpy as np, math, faiss, torch\n",
        "\n",
        "# 1) Créer les exemples d'entraînement (in-batch negatives)\n",
        "train_examples = [InputExample(texts=[q, p])\n",
        "                  for q, p in zip(splits[\"train\"][\"query\"], splits[\"train\"][\"positive\"])]\n",
        "\n",
        "# 2) DataLoader\n",
        "BATCH_SIZE = 64  # baisse à 32 si OOM\n",
        "train_loader = DataLoader(train_examples, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "# 3) Evaluator \"IR\" sur DEV (retrieval réaliste)\n",
        "#    On crée un mini-corpus dev et on mappe queries -> passage id\n",
        "def build_ir_evaluator_from_dev(splits_dev, name=\"dev-ir\"):\n",
        "    # Corpus = uniques des positives\n",
        "    corpus = list(set(splits_dev[\"positive\"]))\n",
        "    # Dictionnaires au format attendu par InformationRetrievalEvaluator\n",
        "    corpus_dict = {str(i): c for i, c in enumerate(corpus)}\n",
        "    queries_dict = {str(i): q for i, q in enumerate(splits_dev[\"query\"])}\n",
        "\n",
        "    # Relevant docs: pour chaque query i, doc id correspondant dans le corpus\n",
        "    def norm(s): return \" \".join(s.split()).strip()\n",
        "    inv = {norm(c): str(i) for i, c in enumerate(corpus)}\n",
        "    relevant_docs = {}\n",
        "    miss = 0\n",
        "    for i, gold in enumerate(splits_dev[\"positive\"]):\n",
        "        gi = inv.get(norm(gold))\n",
        "        if gi is None:\n",
        "            miss += 1\n",
        "        else:\n",
        "            relevant_docs[str(i)] = {gi: 1}\n",
        "    if miss:\n",
        "        print(f\"[dev evaluator] {miss} gold non retrouvés dans le corpus (après normalisation).\")\n",
        "\n",
        "    # Evaluator avec métriques Recall@k, MAP, MRR, NDCG etc.\n",
        "    return evaluation.InformationRetrievalEvaluator(\n",
        "        queries=queries_dict,\n",
        "        corpus=corpus_dict,\n",
        "        relevant_docs=relevant_docs,\n",
        "        show_progress_bar=True,\n",
        "        mrr_at_k=[10],\n",
        "        ndcg_at_k=[10],\n",
        "        accuracy_at_k=[1,5,10],\n",
        "        recall_at_k=[1,5,10],\n",
        "        map_at_k=[10],\n",
        "        name=name\n",
        "    )\n",
        "\n",
        "dev_evaluator = build_ir_evaluator_from_dev(splits[\"dev\"], name=\"dev-ir\")"
      ],
      "metadata": {
        "id": "-5zYc0SNi9TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.2 Charger BGE-M3 + activer LoRA"
      ],
      "metadata": {
        "id": "Pmq8rVGNmSRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "MODEL_NAME = \"BAAI/bge-m3\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer(MODEL_NAME, device=device)\n",
        "model.max_seq_length = 384   # plus rapide; remets 512 pour le run “officiel”\n",
        "\n",
        "# --- Activer LoRA via PEFT (option recommandé) ---\n",
        "USE_LORA = True  # mets False pour fine-tuning \"classique\" si tu veux comparer\n",
        "\n",
        "if USE_LORA:\n",
        "    try:\n",
        "        from peft import LoraConfig, get_peft_model, TaskType\n",
        "        # Règle LoRA: r/alpha/dropout — tu peux ajuster\n",
        "        lora_cfg = LoraConfig(\n",
        "            task_type=TaskType.FEATURE_EXTRACTION,  # embeddings\n",
        "            r=16, lora_alpha=32, lora_dropout=0.1,\n",
        "            # Cible des modules linéaires des blocs d'attention/FFN.\n",
        "            # \"all-linear\" marche bien quand on ne connait pas les noms exacts.\n",
        "            target_modules=\"all-linear\"\n",
        "        )\n",
        "        # Récupérer le backbone HF et l'envelopper avec PEFT\n",
        "        hf_backbone = model._first_module().auto_model\n",
        "        peft_backbone = get_peft_model(hf_backbone, lora_cfg)\n",
        "        peft_backbone.print_trainable_parameters()\n",
        "        # Remettre le backbone LoRA dans SentenceTransformer\n",
        "        model._first_module().auto_model = peft_backbone\n",
        "        print(\"✅ LoRA activé via PEFT.\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Impossible d'activer LoRA, on passe en FT classique. Raison:\", repr(e))\n",
        "        USE_LORA = False\n",
        "\n",
        "# Perfs GPU\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "fqe4f5dNmVTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.3 Définir la loss et les hyperparamètres, lancer l’entraînement"
      ],
      "metadata": {
        "id": "KVjf4XUamYV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import losses\n",
        "\n",
        "# Loss in-batch negatives: pour chaque pair (q, p), les autres p du batch jouent le rôle de négatifs\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
        "\n",
        "EPOCHS = 1              # commence à 1; passe à 2–3 si tu as le temps\n",
        "LR = 2e-5               # 2e-5/3e-5 sont de bons points de départ\n",
        "WARMUP_RATIO = 0.1      # 10% des steps\n",
        "EVAL_STEPS = 500        # éval dev régulière\n",
        "\n",
        "# Calcul des steps\n",
        "num_train_steps = (len(train_loader) * EPOCHS)\n",
        "warmup_steps = int(num_train_steps * WARMUP_RATIO)\n",
        "\n",
        "OUTPUT_DIR = \"./bge-m3-legalkit-lora\" if USE_LORA else \"./bge-m3-legalkit-ft\"\n",
        "\n",
        "# (Optionnel) W&B pour tracer la courbe de loss + scores dev\n",
        "import wandb\n",
        "USE_WANDB = True\n",
        "if USE_WANDB:\n",
        "    wandb.login()\n",
        "    run = wandb.init(\n",
        "        project=\"legal-embeddings\",\n",
        "        name=(\"bge-m3_lora_ft_v1\" if USE_LORA else \"bge-m3_ft_v1\"),\n",
        "        group=\"bge-m3_legalkit\",\n",
        "        config={\n",
        "            \"model\": MODEL_NAME,\n",
        "            \"use_lora\": USE_LORA,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"lr\": LR,\n",
        "            \"warmup_ratio\": WARMUP_RATIO,\n",
        "            \"max_seq_len\": model.max_seq_length\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Callback simple pour logger la loss step sur W&B\n",
        "def wandb_callback(score, epoch, steps):\n",
        "    if USE_WANDB:\n",
        "        if isinstance(score, dict):\n",
        "            wandb.log({f\"dev/{k}\": v for k, v in score.items()}, step=steps)\n",
        "        else:\n",
        "            wandb.log({\"dev/score\": score}, step=steps)\n",
        "\n",
        "# Entraînement\n",
        "model.fit(\n",
        "    train_objectives=[(train_loader, train_loss)],\n",
        "    epochs=EPOCHS,\n",
        "    warmup_steps=warmup_steps,\n",
        "    scheduler=\"cosine\",\n",
        "    optimizer_params={\"lr\": LR},\n",
        "    show_progress_bar=True,\n",
        "    use_amp=True,                 # mixed-precision sur GPU\n",
        "    evaluator=dev_evaluator,      # évalue sur dev pendant l'entraînement\n",
        "    evaluation_steps=EVAL_STEPS,\n",
        "    output_path=OUTPUT_DIR,\n",
        "    save_best_model=True,\n",
        "    callback=wandb_callback if USE_WANDB else None\n",
        ")\n",
        "\n",
        "if USE_WANDB:\n",
        "    run.finish()"
      ],
      "metadata": {
        "id": "4rHKPKj-mZ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.4 Ré-évaluer après entraînement sur le TEST (mêmes métriques que zéro-shot)"
      ],
      "metadata": {
        "id": "jl2yAqbdmcuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Charger le modèle finetuné\n",
        "ft_model_path = OUTPUT_DIR  # celui qu'on vient d'entraîner (best model)\n",
        "ft_model = SentenceTransformer(ft_model_path, device=device)\n",
        "ft_model.max_seq_length = 384\n",
        "\n",
        "# 2) Construire corpus et queries du TEST\n",
        "test_q = list(splits[\"test\"][\"query\"])\n",
        "test_p = list(splits[\"test\"][\"positive\"])\n",
        "corpus  = list(set(test_p))\n",
        "\n",
        "# 3) Encodage\n",
        "bs = 256 if device==\"cuda\" else 32\n",
        "corpus_emb = ft_model.encode(corpus, batch_size=bs, normalize_embeddings=True, show_progress_bar=True)\n",
        "query_emb  = ft_model.encode(test_q, batch_size=bs, normalize_embeddings=True, show_progress_bar=True)\n",
        "\n",
        "# 4) FAISS search\n",
        "index = faiss.IndexFlatIP(corpus_emb.shape[1]); index.add(corpus_emb)\n",
        "_, I = index.search(query_emb, 10)\n",
        "\n",
        "# 5) Gold indices + métriques (comme avant)\n",
        "def norm(s): return \" \".join(s.split()).strip()\n",
        "corpus_norm = [norm(x) for x in corpus]\n",
        "gold_map = {corpus_norm[i]: i for i in range(len(corpus))}\n",
        "golds = [gold_map.get(norm(g), -1) for g in test_p]\n",
        "\n",
        "def recall_at_k(I, k):\n",
        "    hits=0; total=0\n",
        "    for r, gi in enumerate(golds):\n",
        "        if gi==-1: continue\n",
        "        total+=1\n",
        "        if gi in I[r,:k]: hits+=1\n",
        "    return hits/max(1,total)\n",
        "\n",
        "def mrr_at_k(I, k=10):\n",
        "    tot=0.0; n=0\n",
        "    for r,gi in enumerate(golds):\n",
        "        if gi==-1: continue\n",
        "        n+=1; row=list(I[r,:k])\n",
        "        if gi in row: tot += 1.0/(row.index(gi)+1)\n",
        "    return tot/max(1,n)\n",
        "\n",
        "def ndcg_at_k(I, k=10):\n",
        "    tot=0.0; n=0\n",
        "    for r,gi in enumerate(golds):\n",
        "        if gi==-1: continue\n",
        "        n+=1\n",
        "        gains=[1.0 if j==gi else 0.0 for j in I[r,:k]]\n",
        "        import math\n",
        "        dcg=sum(g/math.log2(i+2) for i,g in enumerate(gains))\n",
        "        tot+=dcg\n",
        "    return tot/max(1,n)\n",
        "\n",
        "R1, R5, R10 = recall_at_k(I,1), recall_at_k(I,5), recall_at_k(I,10)\n",
        "MRR10, NDCG10 = mrr_at_k(I,10), ndcg_at_k(I,10)\n",
        "\n",
        "print({\"FT/R@1\":round(R1,3),\"FT/R@5\":round(R5,3),\"FT/R@10\":round(R10,3),\n",
        "       \"FT/MRR@10\":round(MRR10,3),\"FT/nDCG@10\":round(NDCG10,3)})"
      ],
      "metadata": {
        "id": "RSFQvadnmh79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}